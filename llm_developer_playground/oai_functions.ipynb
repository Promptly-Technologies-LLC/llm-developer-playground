{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dir_diary.datastructures import FileClassificationList\n",
    "from dir_diary.client import LLMClient\n",
    "from typing import Optional, Literal\n",
    "import json\n",
    "import openai\n",
    "from llm_cost_estimation import models\n",
    "\n",
    "\n",
    "# Functions to enforce structured output from the chatbot\n",
    "functions = [\n",
    "        {\n",
    "          \"name\": \"classify_project_files_by_role\",\n",
    "          \"description\": \"Identify the role that each file plays in a software project\",\n",
    "          \"parameters\": FileClassificationList.model_json_schema()\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Prompt template for determining the roles that files play in the project\n",
    "file_classification_prompt = (\n",
    "    \"We have mapped the file structure of a project folder for an existing \"\n",
    "    \"coding project. Based solely on the file structure, let's attempt to \"\n",
    "    \"classify them by the role they play in the project. We will label code \"\n",
    "    \"modules, entry points, and endpoints as 'source'; config files, \"\n",
    "    \"environment files, and dependency files as 'configuration'; build files, \"\n",
    "    \"Docker files, and CI/CD files as 'build or deployment'; READMEs, \"\n",
    "    \"CHANGELOGs, pseudocodes, project maps, licenses, and docs as \"\n",
    "    \"'documentation'; unit tests as 'testing'; migration, schema, and seed \"\n",
    "    \"files as 'database', utility and action scripts as 'utility scripts', \"\n",
    "    \"static assets like images, CSS, CSV, and JSON files as 'assets and \"\n",
    "    \"data', and anything else that doesn't fit these categories (e.g., \"\n",
    "    \"compiled distribution files) as 'specialized'. Some files may already \"\n",
    "    \"be classified and included for context. They need not be reclassified \"\n",
    "    \"unless a classification is obviously wrong. 'None' or 'null' values, \"\n",
    "    \"however, should be replaced with the correct role.\\n\"\n",
    "    \"Here is the map of the project file structure:\\n%s\"\n",
    ")\n",
    "\n",
    "def classify_with_openai(input_str: str) -> FileClassificationList:\n",
    "    # Query the LLM to update the project map\n",
    "    project_map: list[dict[str]] = query_llm(\n",
    "                prompt=file_classification_prompt % input_str,\n",
    "                functions=functions\n",
    "            )\n",
    "\n",
    "    # Create a FileClassificationList from the project map\n",
    "    json_project_map: str = json.loads(s=project_map.choices[0][\"message\"][\"function_call\"][\"arguments\"])\n",
    "    parsed_project_map: FileClassificationList = FileClassificationList.model_validate(obj=json_project_map)\n",
    "\n",
    "    # Return the project map\n",
    "    return parsed_project_map\n",
    "\n",
    "\n",
    "# Prompt to generate a pseudocode summary of a code module\n",
    "pseudocode_prompt = (\n",
    "    \"Generate an abbreviated natural-language pseudocode summary of the \"\n",
    "    \"following code. Make sure to include function, class, and argument names \"\n",
    "    \"and to indicate where objects are imported from so a reader can \"\n",
    "    \"understand the execution context and usage. Well-formatted pseudocode \"\n",
    "    \"will separate object and function blocks with a blank line and will use \"\n",
    "    \"hierarchical ordered and unordered lists to show execution sequence and \"\n",
    "    \"logical relationships.\\nHere is the code to summarize:\\n%s\"\n",
    ")\n",
    "\n",
    "# Prompt to generate a usage summary of a code module\n",
    "usage_prompt = (\n",
    "    \"Generate natural-language instructions on how to use the following code. \"\n",
    "    \"Describe what the code is doing, how to create instances or invoke \"\n",
    "    \"methods of defined objects, and how to invoke functions. As much as \"\n",
    "    \"possible, infer what data types are expected by function arguments and \"\n",
    "    \"class methods, as well as what data types are returned. When usage \"\n",
    "    \"cannot be inferred for types and classes imported from outside this \"\n",
    "    \"module, flag the uncertainties and indicate where they are imported \"\n",
    "    \"from. Well-formatted usage summaries will separate instructions for \"\n",
    "    \"different objects and functions with a blank line.\\nHere is the code to \"\n",
    "    \"summarize:\\n%s\"\n",
    ")\n",
    "\n",
    "def summarize_with_openai(input_str: str, summary_type: Literal[\"pseudocode\", \"usage\"]) -> str:\n",
    "    # Determine the prompt to use\n",
    "    if summary_type == \"pseudocode\":\n",
    "        prompt = usage_prompt\n",
    "    elif summary_type == \"usage\":\n",
    "        prompt = pseudocode_prompt\n",
    "\n",
    "    # Query the chatbot for a summary and parse the output\n",
    "    generated_summary: str = query_llm(\n",
    "                prompt=prompt % input_str\n",
    "            )\n",
    "    \n",
    "    generated_summary.choices[0][\"choices\"][\"message\"][\"content\"]\n",
    "    \n",
    "    return generated_summary\n",
    "\n",
    "\n",
    "# Get max_tokens based on model_name\n",
    "def get_max_tokens(long: bool = False) -> int:\n",
    "    # Initialize LLMClient to get config and track cost\n",
    "    client = LLMClient()\n",
    "    \n",
    "    # Set model_name based on config + `long` argument\n",
    "    model_name = client.model_name if long else client.long_context_fallback\n",
    "    \n",
    "    # Set max_tokens based on model_name\n",
    "    if \"32k\" in model_name:\n",
    "        max_tokens = 16000\n",
    "    elif \"16k\" in model_name:\n",
    "        max_tokens = 8000\n",
    "    elif \"gpt-4\" in model_name:\n",
    "        max_tokens = 4000\n",
    "    else:\n",
    "        max_tokens = 2000\n",
    "\n",
    "    # Return the chatbot instance\n",
    "    return max_tokens\n",
    "\n",
    "\n",
    "# Query a chatbot using a prompt, and optionally a functions list\n",
    "def query_llm(prompt: str, functions: Optional[list[dict]] = None) -> str:\n",
    "    # Initialize the client\n",
    "    client = LLMClient()\n",
    "    openai.api_key = client.api_key\n",
    "\n",
    "    # Generate the output from the input\n",
    "    try:\n",
    "        model_used = client.model_name\n",
    "        response: dict = openai.ChatCompletion.create(\n",
    "            model=client.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            functions=functions\n",
    "        )\n",
    "    except openai.InvalidRequestError as e:\n",
    "        # If we exceed context limit, check if long_context_fallback is None\n",
    "        if client.long_context_fallback is None:\n",
    "            # If long_context_fallback is None, raise the error\n",
    "            raise e\n",
    "        else:\n",
    "            # If long_context_fallback is not None, warn and use long_context_fallback\n",
    "            print(\"Encountered error:\\n\" + e + \"\\nTrying again with long_context_fallback.\")\n",
    "            model_used = client.long_context_fallback\n",
    "            response: dict = openai.ChatCompletion.create(\n",
    "                model=client.long_context_fallback,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                functions=functions\n",
    "            )\n",
    "    \n",
    "    # Update the total cost\n",
    "    client.total_cost += calculate_cost(\n",
    "            prompt_tokens=response[\"usage\"][\"prompt_tokens\"],\n",
    "            completion_tokens=response[\"usage\"][\"completion_tokens\"],\n",
    "            model_name=model_used\n",
    "        )\n",
    "\n",
    "    # Return the response object\n",
    "    return response\n",
    "\n",
    "\n",
    "# Calculate cost of a query\n",
    "def calculate_cost(prompt_tokens: int, completion_tokens: int, model_used: str) -> float:\n",
    "    # Get cost per token for the model used from the models object\n",
    "    prompt_cost_per_token = [model['prompt_cost_per_token'] for model in models if model['name'] == model_used][0]\n",
    "    completion_cost_per_token = [model['completion_cost_per_token'] for model in models if model['name'] == model_used][0]\n",
    "    \n",
    "    # Calculcate and return the total cost of the query\n",
    "    total_cost = (prompt_tokens * prompt_cost_per_token) + (completion_tokens * completion_cost_per_token)\n",
    "    return total_cost\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-developer-playground-6LGp-str",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
