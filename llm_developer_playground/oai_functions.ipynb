{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function calling with the `openai` Python library\n",
    "\n",
    "This notebook demonstrates how to create a reusable `query_llm` function that allows calling the OpenAI Chat Completions endpoint with optional arbitrary function-calling argumentsâ€”to return either structured data or unstructured text.\n",
    "\n",
    "Note that OpenAI function calling is designed to work with Pydantic models. You specify a Pydantic model to describe the structured data you want, and the function will return JSON corresponding to that model. You can then convert the JSON to an instance of the model using the Pydantic model's `.model_validate()` method.\n",
    "\n",
    "Note that we also extend the OpenAI client to allow for specifying the LLM `model` and `fallback` model up front. This allows for the model to be specified when initializing the client, rather than every time we call the LLM. We also add a `total_cost` attribute to allow for tracking the cumulative cost of our API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"files\":[{\"path\":\".gitignore\",\"role\":\"configuration\"},{\"path\":\"poetry.lock\",\"role\":\"configuration\"},{\"path\":\"pyproject.toml\",\"role\":\"configuration\"},{\"path\":\"README.md\",\"role\":\"documentation\"},{\"path\":\"LICENSE\",\"role\":\"documentation\"},{\"path\":\"tests/__init__.py\",\"role\":\"testing\"},{\"path\":\"tests/test_file_handler.py\",\"role\":\"testing\"},{\"path\":\"file_handler/file_handler.py\",\"role\":\"source\"}]}', name='classify_project_files_by_role'), tool_calls=None))\n",
      "{\n",
      "    \"files\": [\n",
      "        {\n",
      "            \"path\": \".gitignore\",\n",
      "            \"role\": \"configuration\"\n",
      "        },\n",
      "        {\n",
      "            \"path\": \"poetry.lock\",\n",
      "            \"role\": \"configuration\"\n",
      "        },\n",
      "        {\n",
      "            \"path\": \"pyproject.toml\",\n",
      "            \"role\": \"configuration\"\n",
      "        },\n",
      "        {\n",
      "            \"path\": \"README.md\",\n",
      "            \"role\": \"documentation\"\n",
      "        },\n",
      "        {\n",
      "            \"path\": \"LICENSE\",\n",
      "            \"role\": \"documentation\"\n",
      "        },\n",
      "        {\n",
      "            \"path\": \"tests\\\\__init__.py\",\n",
      "            \"role\": \"testing\"\n",
      "        },\n",
      "        {\n",
      "            \"path\": \"tests\\\\test_file_handler.py\",\n",
      "            \"role\": \"testing\"\n",
      "        },\n",
      "        {\n",
      "            \"path\": \"file_handler\\\\file_handler.py\",\n",
      "            \"role\": \"source\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from os import getenv\n",
    "from pathlib import Path\n",
    "from typing import Optional, Literal\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI, BadRequestError\n",
    "from openai.types.chat import ChatCompletion\n",
    "\n",
    "# Data structure for LLM project map response (use Literal rather than Enum\n",
    "# because it plays nicer with LLM prompting and JSON serialization\n",
    "class FileClassification(BaseModel):\n",
    "    path: Path = Field(description=\"file path relative to the project root\")\n",
    "    role: Literal[\"source\", \"configuration\", \"build or deployment\", \"documentation\", \"testing\", \"database\", \"utility scripts\", \"assets or data\", \"specialized\"] = Field(\n",
    "            default=None, description=\"role the file plays in the project\"\n",
    "        )\n",
    "\n",
    "# Data structure for a list of FileClassifications\n",
    "class FileClassificationList(BaseModel):\n",
    "    files: list[FileClassification] = Field(\n",
    "            description=\"List of file classifications\"\n",
    "        )\n",
    "\n",
    "    # Method to convert a FileClassificationList to a JSON-formatted string\n",
    "    def to_json(self) -> str:\n",
    "        data_dict = self.model_dump(exclude_unset=True)\n",
    "\n",
    "        # Convert Path objects to str\n",
    "        for file in data_dict.get(\"files\", []):\n",
    "            file[\"path\"] = str(object=file[\"path\"])\n",
    "\n",
    "        return json.dumps(obj=data_dict, indent=4)\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Extend the OpenAI class to include, total_cost, model, and fallback attributes\n",
    "# (total_cost is used to track the total cost of queries, model is the model used,\n",
    "# and fallback is the model to use if the context window is exceeded)\n",
    "class ExtendedOpenAI(OpenAI):\n",
    "    def __init__(self, *args, model=\"gpt-3.5-turbo\", fallback=\"gpt-4-turbo\", **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.total_cost = 0.0\n",
    "        self.model = model\n",
    "        self.fallback = fallback\n",
    "\n",
    "# Create an ExtendedOpenAI client instance\n",
    "client = ExtendedOpenAI(\n",
    "  api_key=getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# Models and their costs\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"gpt-3.5-turbo\",\n",
    "        \"max_tokens\": 16385,\n",
    "        \"prompt_cost_per_token\": 0.5 / 1000000,\n",
    "        \"completion_cost_per_token\": 1.5 / 1000000,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gpt-4-turbo\",\n",
    "        \"max_tokens\": 128000,\n",
    "        \"prompt_cost_per_token\": 10 / 1000000,\n",
    "        \"completion_cost_per_token\": 30 / 1000000,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gpt-4\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"prompt_cost_per_token\": 30 / 1000000,\n",
    "        \"completion_cost_per_token\": 60 / 1000000,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gpt-4-32k\",\n",
    "        \"max_tokens\": 32768,\n",
    "        \"prompt_cost_per_token\": 30 / 1000000,\n",
    "        \"completion_cost_per_token\": 0.12 / 1000000,\n",
    "    }\n",
    "]\n",
    "\n",
    "# Functions to enforce structured output from the chatbot\n",
    "functions = [\n",
    "        {\n",
    "          \"name\": \"classify_project_files_by_role\",\n",
    "          \"description\": \"Identify the role that each file plays in a software project\",\n",
    "          \"parameters\": FileClassificationList.model_json_schema()\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Prompt template for determining the roles that files play in the project\n",
    "file_classification_prompt = (\n",
    "    \"We have mapped the file structure of a project folder for an existing \"\n",
    "    \"coding project. Based solely on the file structure, let's attempt to \"\n",
    "    \"classify them by the role they play in the project. We will label code \"\n",
    "    \"modules, entry points, and endpoints as 'source'; config files, \"\n",
    "    \"environment files, and dependency files as 'configuration'; build files, \"\n",
    "    \"Docker files, and CI/CD files as 'build or deployment'; READMEs, \"\n",
    "    \"CHANGELOGs, pseudocodes, project maps, licenses, and docs as \"\n",
    "    \"'documentation'; unit tests as 'testing'; migration, schema, and seed \"\n",
    "    \"files as 'database', utility and action scripts as 'utility scripts', \"\n",
    "    \"static assets like images, CSS, CSV, and JSON files as 'assets and \"\n",
    "    \"data', and anything else that doesn't fit these categories (e.g., \"\n",
    "    \"compiled distribution files) as 'specialized'.\\n\"\n",
    "    \"Here is the map of the project file structure:\\n%s\"\n",
    ")\n",
    "\n",
    "def classify_with_openai(input_str: str) -> FileClassificationList:\n",
    "    # Query the LLM to update the project map\n",
    "    project_map: list[dict[str]] = query_llm(\n",
    "                prompt=file_classification_prompt % input_str,\n",
    "                functions=functions\n",
    "            )\n",
    "\n",
    "    # Create a FileClassificationList from the project map\n",
    "    print(project_map.choices[0])\n",
    "    json_project_map: str = json.loads(s=project_map.choices[0].message.function_call.arguments)\n",
    "    parsed_project_map: FileClassificationList = FileClassificationList.model_validate(obj=json_project_map)\n",
    "\n",
    "    # Return the project map\n",
    "    return parsed_project_map\n",
    "\n",
    "\n",
    "# Get max_tokens based on model_name\n",
    "def get_max_tokens(long: bool = False) -> int:\n",
    "    # Set model_name based on config + `long` argument\n",
    "    model_name = client.model if long else client.fallback\n",
    "    \n",
    "    # Set max_tokens based on model_name\n",
    "    max_tokens = [model['max_tokens'] for model in models if model['name'] in model_name][0]\n",
    "\n",
    "    # Return the chatbot instance\n",
    "    return max_tokens\n",
    "\n",
    "\n",
    "# Calculate cost of a query\n",
    "def calculate_cost(prompt_tokens: int, completion_tokens: int, model_used: str) -> float:\n",
    "    # Get cost per token for the model used from the models object\n",
    "    prompt_cost_per_token = [model['prompt_cost_per_token'] for model in models if model['name'] == model_used][0]\n",
    "    completion_cost_per_token = [model['completion_cost_per_token'] for model in models if model['name'] == model_used][0]\n",
    "    \n",
    "    # Calculcate and return the total cost of the query\n",
    "    total_cost = (prompt_tokens * prompt_cost_per_token) + (completion_tokens * completion_cost_per_token)\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "# Query a chatbot using a prompt, and optionally a functions list\n",
    "def query_llm(prompt: str, functions: Optional[list[dict]] = None) -> str:\n",
    "    # Generate the output from the input\n",
    "    try:\n",
    "        model_used: str = client.model\n",
    "        response: ChatCompletion = client.chat.completions.create(\n",
    "            model=client.model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            functions=functions\n",
    "        )\n",
    "    except BadRequestError as e:\n",
    "        # If we exceed context limit, check if long_context_fallback is None\n",
    "        if client.fallback is None:\n",
    "            # If long_context_fallback is None, raise the error\n",
    "            raise e\n",
    "        else:\n",
    "            # If long_context_fallback is not None, warn and use long_context_fallback\n",
    "            print(\"Encountered error:\\n\" + e + \"\\nTrying again with long_context_fallback.\")\n",
    "            model_used: str = client.fallback\n",
    "            response: ChatCompletion = client.chat.completions.create(\n",
    "                model=client.fallback,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                functions=functions\n",
    "            )\n",
    "    \n",
    "    # Update the total cost\n",
    "    client.total_cost += calculate_cost(\n",
    "            prompt_tokens=response.usage.prompt_tokens,\n",
    "            completion_tokens=response.usage.completion_tokens,\n",
    "            model_used=model_used\n",
    "        )\n",
    "\n",
    "    # Return the response object\n",
    "    return response\n",
    "\n",
    "sample_project_map: dict[list[dict]] = {\"files\": [\n",
    "    {\"path\": \".gitignore\", \"role\": None},\n",
    "    {\"path\": \"poetry.lock\", \"role\": None},\n",
    "    {\"path\": \"pyproject.toml\", \"role\": None},\n",
    "    {\"path\": \"README.md\", \"role\": None},\n",
    "    {\"path\": \"LICENSE\", \"role\": None},\n",
    "    {\"path\": \"tests\\\\__init__.py\", \"role\": None},\n",
    "    {\"path\": \"tests\\\\test_file_handler.py\", \"role\": None},\n",
    "    {\"path\": \"file_handler\\\\file_handler.py\", \"role\": None},\n",
    "]}\n",
    "\n",
    "# Convert the file list to a string\n",
    "input_str: str = \", \".join([item[\"path\"] for item in sample_project_map[\"files\"]])\n",
    "\n",
    "# Classify the project files by role\n",
    "file_classifications: FileClassificationList = classify_with_openai(input_str=input_str)\n",
    "\n",
    "# Print the file classifications\n",
    "print(file_classifications.to_json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we wrote our `query_llm` function in such a way that it can flexibly be used for calling arbitrary functions, as above, or for interacting with the LLM in the more usual way with text queries, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: \n",
      "- Name: get_fibonacci_numbers_below_n\n",
      "- Argument: \n",
      "  - n (int)\n",
      "- Return: \n",
      "  - list[int]\n",
      "\n",
      "---\n",
      "\n",
      "1. Start\n",
      "2. Create a list called fibonacci_sequence with initial values [1, 2]\n",
      "3. While the last element in fibonacci_sequence is less than n:\n",
      "    1. Append the sum of the last two elements in fibonacci_sequence to the list\n",
      "4. Return fibonacci_sequence\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Prompt to generate a pseudocode summary of a code module\n",
    "pseudocode_prompt = (\n",
    "    \"Generate an abbreviated natural-language pseudocode summary of the \"\n",
    "    \"following code. Make sure to include function, class, and argument names \"\n",
    "    \"and to indicate where objects are imported from so a reader can \"\n",
    "    \"understand the execution context and usage. Well-formatted pseudocode \"\n",
    "    \"will separate object and function blocks with a blank line and will use \"\n",
    "    \"hierarchical ordered and unordered lists to show execution sequence and \"\n",
    "    \"logical relationships.\\nHere is the code to summarize:\\n%s\"\n",
    ")\n",
    "\n",
    "sample_code = '''\n",
    "def get_fibonacci_numbers_below_n(n) -> list[int]:\n",
    "  fibonacci_sequence = [1, 2]\n",
    "  while fibonacci_sequence[len(fibonacci_sequence)-1] < n:\n",
    "    fibonacci_sequence.append(sum(\n",
    "        fibonacci_sequence[(len(fibonacci_sequence) - 2):len(fibonacci_sequence)]\n",
    "      ))\n",
    "  return fibonacci_sequence\n",
    "  '''\n",
    "\n",
    "def summarize_with_openai(input_str: str) -> str:\n",
    "    # Generate a prompt for the chatbot\n",
    "    prompt = pseudocode_prompt % sample_code\n",
    "\n",
    "    # Query the chatbot for a summary and parse the output\n",
    "    generated_summary: ChatCompletion = query_llm(\n",
    "                prompt=prompt\n",
    "            )\n",
    "    \n",
    "    return generated_summary.choices[0].message.content\n",
    "\n",
    "# Generate a pseudocode summary of the sample code\n",
    "pseudocode_summary: str = summarize_with_openai(input_str=sample_code)\n",
    "\n",
    "# Print the pseudocode summary\n",
    "print(pseudocode_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-developer-playground-6LGp-str",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
