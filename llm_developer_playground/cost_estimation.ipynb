{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the likely cost of an API call\n",
    "\n",
    "I've created a simple Python library to assist with estimating the cost of an API call before making the call. The library can be installed with `pip install llm_cost_estimation`. It exports the following objects:\n",
    "\n",
    "- models: Contains essential details about various LLMs, including cost per prompt token, cost per completion token, model description, and maximum allowed context length (in tokens).\n",
    "- count_token: A utility function to count the tokens present in a specific prompt or chat history using a given model's encoding system.\n",
    "- estimate_costs: A utility function to provide cost estimates for API calls to specified LLMs, based on selected model and length of text prompt or average length of messages in chat history.\n",
    "\n",
    "# Fetching a summary of model costs\n",
    "\n",
    "To view the models object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0254a th {\n",
       "  max-width: 85px;\n",
       "  word-break: break-all;\n",
       "}\n",
       "#T_0254a_row0_col0, #T_0254a_row0_col2, #T_0254a_row0_col3, #T_0254a_row0_col4, #T_0254a_row1_col0, #T_0254a_row1_col2, #T_0254a_row1_col3, #T_0254a_row1_col4, #T_0254a_row2_col0, #T_0254a_row2_col2, #T_0254a_row2_col3, #T_0254a_row2_col4, #T_0254a_row3_col0, #T_0254a_row3_col2, #T_0254a_row3_col3, #T_0254a_row3_col4, #T_0254a_row4_col0, #T_0254a_row4_col2, #T_0254a_row4_col3, #T_0254a_row4_col4, #T_0254a_row5_col0, #T_0254a_row5_col2, #T_0254a_row5_col3, #T_0254a_row5_col4, #T_0254a_row6_col0, #T_0254a_row6_col2, #T_0254a_row6_col3, #T_0254a_row6_col4, #T_0254a_row7_col0, #T_0254a_row7_col2, #T_0254a_row7_col3, #T_0254a_row7_col4, #T_0254a_row8_col0, #T_0254a_row8_col2, #T_0254a_row8_col3, #T_0254a_row8_col4, #T_0254a_row9_col0, #T_0254a_row9_col2, #T_0254a_row9_col3, #T_0254a_row9_col4, #T_0254a_row10_col0, #T_0254a_row10_col2, #T_0254a_row10_col3, #T_0254a_row10_col4, #T_0254a_row11_col0, #T_0254a_row11_col2, #T_0254a_row11_col3, #T_0254a_row11_col4, #T_0254a_row12_col0, #T_0254a_row12_col2, #T_0254a_row12_col3, #T_0254a_row12_col4, #T_0254a_row13_col0, #T_0254a_row13_col2, #T_0254a_row13_col3, #T_0254a_row13_col4, #T_0254a_row14_col0, #T_0254a_row14_col2, #T_0254a_row14_col3, #T_0254a_row14_col4, #T_0254a_row15_col0, #T_0254a_row15_col2, #T_0254a_row15_col3, #T_0254a_row15_col4, #T_0254a_row16_col0, #T_0254a_row16_col2, #T_0254a_row16_col3, #T_0254a_row16_col4, #T_0254a_row17_col0, #T_0254a_row17_col2, #T_0254a_row17_col3, #T_0254a_row17_col4, #T_0254a_row18_col0, #T_0254a_row18_col2, #T_0254a_row18_col3, #T_0254a_row18_col4, #T_0254a_row19_col0, #T_0254a_row19_col2, #T_0254a_row19_col3, #T_0254a_row19_col4 {\n",
       "  max-width: 80px;\n",
       "}\n",
       "#T_0254a_row0_col1, #T_0254a_row1_col1, #T_0254a_row2_col1, #T_0254a_row3_col1, #T_0254a_row4_col1, #T_0254a_row5_col1, #T_0254a_row6_col1, #T_0254a_row7_col1, #T_0254a_row8_col1, #T_0254a_row9_col1, #T_0254a_row10_col1, #T_0254a_row11_col1, #T_0254a_row12_col1, #T_0254a_row13_col1, #T_0254a_row14_col1, #T_0254a_row15_col1, #T_0254a_row16_col1, #T_0254a_row17_col1, #T_0254a_row18_col1, #T_0254a_row19_col1 {\n",
       "  max-width: 80px;\n",
       "  max-width: 280px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0254a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_0254a_level0_col0\" class=\"col_heading level0 col0\" >completion_cost_per_token</th>\n",
       "      <th id=\"T_0254a_level0_col1\" class=\"col_heading level0 col1\" >description</th>\n",
       "      <th id=\"T_0254a_level0_col2\" class=\"col_heading level0 col2\" >max_tokens</th>\n",
       "      <th id=\"T_0254a_level0_col3\" class=\"col_heading level0 col3\" >name</th>\n",
       "      <th id=\"T_0254a_level0_col4\" class=\"col_heading level0 col4\" >prompt_cost_per_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row0_col0\" class=\"data row0 col0\" >0.002 / 1000</td>\n",
       "      <td id=\"T_0254a_row0_col1\" class=\"data row0 col1\" >Most capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration.</td>\n",
       "      <td id=\"T_0254a_row0_col2\" class=\"data row0 col2\" >4096</td>\n",
       "      <td id=\"T_0254a_row0_col3\" class=\"data row0 col3\" >gpt-3.5-turbo</td>\n",
       "      <td id=\"T_0254a_row0_col4\" class=\"data row0 col4\" >0.002 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row1_col0\" class=\"data row1 col0\" >0.06 / 1000</td>\n",
       "      <td id=\"T_0254a_row1_col1\" class=\"data row1 col1\" >Same capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.</td>\n",
       "      <td id=\"T_0254a_row1_col2\" class=\"data row1 col2\" >32768</td>\n",
       "      <td id=\"T_0254a_row1_col3\" class=\"data row1 col3\" >gpt-4-32k</td>\n",
       "      <td id=\"T_0254a_row1_col4\" class=\"data row1 col4\" >0.12 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row2_col0\" class=\"data row2 col0\" >0.06 / 1000</td>\n",
       "      <td id=\"T_0254a_row2_col1\" class=\"data row2 col1\" >More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration.</td>\n",
       "      <td id=\"T_0254a_row2_col2\" class=\"data row2 col2\" >8192</td>\n",
       "      <td id=\"T_0254a_row2_col3\" class=\"data row2 col3\" >gpt-4</td>\n",
       "      <td id=\"T_0254a_row2_col4\" class=\"data row2 col4\" >0.03 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row3_col0\" class=\"data row3 col0\" >0.0004 / 1000</td>\n",
       "      <td id=\"T_0254a_row3_col1\" class=\"data row3 col1\" >Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.</td>\n",
       "      <td id=\"T_0254a_row3_col2\" class=\"data row3 col2\" >2049</td>\n",
       "      <td id=\"T_0254a_row3_col3\" class=\"data row3 col3\" >text-ada-001</td>\n",
       "      <td id=\"T_0254a_row3_col4\" class=\"data row3 col4\" >0.0004 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row4_col0\" class=\"data row4 col0\" >0.0005 / 1000</td>\n",
       "      <td id=\"T_0254a_row4_col1\" class=\"data row4 col1\" >Capable of straightforward tasks, very fast, and lower cost.</td>\n",
       "      <td id=\"T_0254a_row4_col2\" class=\"data row4 col2\" >2049</td>\n",
       "      <td id=\"T_0254a_row4_col3\" class=\"data row4 col3\" >text-babbage-001</td>\n",
       "      <td id=\"T_0254a_row4_col4\" class=\"data row4 col4\" >0.0005 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row5_col0\" class=\"data row5 col0\" >0.002 / 1000</td>\n",
       "      <td id=\"T_0254a_row5_col1\" class=\"data row5 col1\" >Very capable, faster and lower cost than Davinci.</td>\n",
       "      <td id=\"T_0254a_row5_col2\" class=\"data row5 col2\" >2049</td>\n",
       "      <td id=\"T_0254a_row5_col3\" class=\"data row5 col3\" >text-curie-001</td>\n",
       "      <td id=\"T_0254a_row5_col4\" class=\"data row5 col4\" >0.002 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row6_col0\" class=\"data row6 col0\" >0.02 / 1000</td>\n",
       "      <td id=\"T_0254a_row6_col1\" class=\"data row6 col1\" >None</td>\n",
       "      <td id=\"T_0254a_row6_col2\" class=\"data row6 col2\" >8001</td>\n",
       "      <td id=\"T_0254a_row6_col3\" class=\"data row6 col3\" >text-davinci-001</td>\n",
       "      <td id=\"T_0254a_row6_col4\" class=\"data row6 col4\" >0.02 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row7_col0\" class=\"data row7 col0\" >0.02 / 1000</td>\n",
       "      <td id=\"T_0254a_row7_col1\" class=\"data row7 col1\" >Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning</td>\n",
       "      <td id=\"T_0254a_row7_col2\" class=\"data row7 col2\" >4097</td>\n",
       "      <td id=\"T_0254a_row7_col3\" class=\"data row7 col3\" >text-davinci-002</td>\n",
       "      <td id=\"T_0254a_row7_col4\" class=\"data row7 col4\" >0.02 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row8_col0\" class=\"data row8 col0\" >0.02 / 1000</td>\n",
       "      <td id=\"T_0254a_row8_col1\" class=\"data row8 col1\" >Most capable GPT-3 model. Can do any task the other models can do, often with higher quality.</td>\n",
       "      <td id=\"T_0254a_row8_col2\" class=\"data row8 col2\" >4097</td>\n",
       "      <td id=\"T_0254a_row8_col3\" class=\"data row8 col3\" >text-davinci-003</td>\n",
       "      <td id=\"T_0254a_row8_col4\" class=\"data row8 col4\" >0.02 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row9_col0\" class=\"data row9 col0\" >0.06 / 1000</td>\n",
       "      <td id=\"T_0254a_row9_col1\" class=\"data row9 col1\" >Snapshot of gpt-4 from March 14th 2023. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.</td>\n",
       "      <td id=\"T_0254a_row9_col2\" class=\"data row9 col2\" >8192</td>\n",
       "      <td id=\"T_0254a_row9_col3\" class=\"data row9 col3\" >gpt-4-0314</td>\n",
       "      <td id=\"T_0254a_row9_col4\" class=\"data row9 col4\" >0.03 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row10_col0\" class=\"data row10 col0\" >0.06 / 1000</td>\n",
       "      <td id=\"T_0254a_row10_col1\" class=\"data row10 col1\" >Snapshot of gpt-4-32 from March 14th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.</td>\n",
       "      <td id=\"T_0254a_row10_col2\" class=\"data row10 col2\" >32768</td>\n",
       "      <td id=\"T_0254a_row10_col3\" class=\"data row10 col3\" >gpt-4-32k-0314</td>\n",
       "      <td id=\"T_0254a_row10_col4\" class=\"data row10 col4\" >0.12 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row11_col0\" class=\"data row11 col0\" >0.002 / 1000</td>\n",
       "      <td id=\"T_0254a_row11_col1\" class=\"data row11 col1\" >Snapshot of gpt-3.5-turbo from March 1st 2023. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.</td>\n",
       "      <td id=\"T_0254a_row11_col2\" class=\"data row11 col2\" >4096</td>\n",
       "      <td id=\"T_0254a_row11_col3\" class=\"data row11 col3\" >gpt-3.5-turbo-0301</td>\n",
       "      <td id=\"T_0254a_row11_col4\" class=\"data row11 col4\" >0.002 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row12_col0\" class=\"data row12 col0\" >0.002 / 1000</td>\n",
       "      <td id=\"T_0254a_row12_col1\" class=\"data row12 col1\" >Optimized for code-completion tasks</td>\n",
       "      <td id=\"T_0254a_row12_col2\" class=\"data row12 col2\" >8001</td>\n",
       "      <td id=\"T_0254a_row12_col3\" class=\"data row12 col3\" >code-davinci-002</td>\n",
       "      <td id=\"T_0254a_row12_col4\" class=\"data row12 col4\" >0.002 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row13_col0\" class=\"data row13 col0\" >0.0200 / 1000</td>\n",
       "      <td id=\"T_0254a_row13_col1\" class=\"data row13 col1\" >Most capable GPT-3 model. Can do any task the other models can do, often with higher quality.</td>\n",
       "      <td id=\"T_0254a_row13_col2\" class=\"data row13 col2\" >2049</td>\n",
       "      <td id=\"T_0254a_row13_col3\" class=\"data row13 col3\" >davinci</td>\n",
       "      <td id=\"T_0254a_row13_col4\" class=\"data row13 col4\" >0.002 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row14_col0\" class=\"data row14 col0\" >0.0020 / 1000</td>\n",
       "      <td id=\"T_0254a_row14_col1\" class=\"data row14 col1\" >Very capable, but faster and lower cost than Davinci.</td>\n",
       "      <td id=\"T_0254a_row14_col2\" class=\"data row14 col2\" >2049</td>\n",
       "      <td id=\"T_0254a_row14_col3\" class=\"data row14 col3\" >curie</td>\n",
       "      <td id=\"T_0254a_row14_col4\" class=\"data row14 col4\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row15_col0\" class=\"data row15 col0\" >0.0005 / 1000</td>\n",
       "      <td id=\"T_0254a_row15_col1\" class=\"data row15 col1\" >Capable of straightforward tasks, very fast, and lower cost.</td>\n",
       "      <td id=\"T_0254a_row15_col2\" class=\"data row15 col2\" >2049</td>\n",
       "      <td id=\"T_0254a_row15_col3\" class=\"data row15 col3\" >babbage</td>\n",
       "      <td id=\"T_0254a_row15_col4\" class=\"data row15 col4\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row16_col0\" class=\"data row16 col0\" >0.0004 / 1000</td>\n",
       "      <td id=\"T_0254a_row16_col1\" class=\"data row16 col1\" >Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.</td>\n",
       "      <td id=\"T_0254a_row16_col2\" class=\"data row16 col2\" >2049</td>\n",
       "      <td id=\"T_0254a_row16_col3\" class=\"data row16 col3\" >ada</td>\n",
       "      <td id=\"T_0254a_row16_col4\" class=\"data row16 col4\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row17_col0\" class=\"data row17 col0\" >0.002 / 1000</td>\n",
       "      <td id=\"T_0254a_row17_col1\" class=\"data row17 col1\" >Earlier version of code-davinci-002</td>\n",
       "      <td id=\"T_0254a_row17_col2\" class=\"data row17 col2\" >8001</td>\n",
       "      <td id=\"T_0254a_row17_col3\" class=\"data row17 col3\" >code-davinci-001</td>\n",
       "      <td id=\"T_0254a_row17_col4\" class=\"data row17 col4\" >0.002 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row18_col0\" class=\"data row18 col0\" >None</td>\n",
       "      <td id=\"T_0254a_row18_col1\" class=\"data row18 col1\" >Almost as capable as Davinci Codex, but slightly faster. This speed advantage may make it preferable for real-time applications.</td>\n",
       "      <td id=\"T_0254a_row18_col2\" class=\"data row18 col2\" >2048</td>\n",
       "      <td id=\"T_0254a_row18_col3\" class=\"data row18 col3\" >code-cushman-002</td>\n",
       "      <td id=\"T_0254a_row18_col4\" class=\"data row18 col4\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0254a_row19_col0\" class=\"data row19 col0\" >None</td>\n",
       "      <td id=\"T_0254a_row19_col1\" class=\"data row19 col1\" >Earlier version of code-cushman-002</td>\n",
       "      <td id=\"T_0254a_row19_col2\" class=\"data row19 col2\" >2048</td>\n",
       "      <td id=\"T_0254a_row19_col3\" class=\"data row19 col3\" >code-cushman-001</td>\n",
       "      <td id=\"T_0254a_row19_col4\" class=\"data row19 col4\" >None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b21832eb10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_cost_estimation import models\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "models_df = pd.DataFrame(models)\n",
    "\n",
    "# Display the DataFrame\n",
    "models_df.style\\\n",
    "    .hide(axis=\"index\")\\\n",
    "    .set_properties(**{'max-width': '80px'})\\\n",
    "    .set_properties(subset=['description'], **{'max-width': '280px'})\\\n",
    "    .set_table_styles([dict(selector=\"th\",props=[('max-width', '85px'),('word-break', 'break-all')])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting tokens in a prompt and estimating completion tokens\n",
    "\n",
    "The `count_tokens` function counts tokens in a prompt and makes a crude estimate of completion tokens. The estimate of completion tokens is based on the simple heuristic that the completion is likely to be of similar length to either the prompt or previous messages in the conversation. This heuristic is fast, but it may not be all that accurate. I'd love to have somebody [contribute](https://github.com/chriscarrollsmith/llm-cost-estimator) an optional alterantive estimation method, such as using a real-world measured prompt length to completion length ratio, or asking a very low-cost model like text-ada-001 guess how long the completion might be.\n",
    "\n",
    "To use the `count_tokens` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt: 6\n",
      "Estimated number of tokens in the completion: 6\n"
     ]
    }
   ],
   "source": [
    "from llm_cost_estimation import count_tokens\n",
    "\n",
    "text = \"Hello, how are you?\"\n",
    "model = \"gpt-4\"\n",
    "\n",
    "# Count tokens in the text\n",
    "prompt_tokens, estimated_completion_tokens = count_tokens(text, model)\n",
    "\n",
    "print(f\"Number of tokens in the prompt: {prompt_tokens}\")\n",
    "print(f\"Estimated number of tokens in the completion: {estimated_completion_tokens}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating cost of a prompt + completion\n",
    "\n",
    "The `estimate_cost` function tries to guess what the cost of a text completion will be, using `count_tokens` on its backend as a helper function.\n",
    "\n",
    "To use the `estimate_cost` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost of this completion: 0.0005399999999999999\n"
     ]
    }
   ],
   "source": [
    "from llm_cost_estimation import estimate_cost\n",
    "\n",
    "prompt = \"Hello, how are you?\"\n",
    "model = \"gpt-4\"\n",
    "\n",
    "# Estimate the cost for the completion\n",
    "estimated_cost = estimate_cost(prompt, model)\n",
    "\n",
    "print(f\"Estimated cost of this completion: {estimated_cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
