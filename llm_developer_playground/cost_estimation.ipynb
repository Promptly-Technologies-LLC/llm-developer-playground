{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the likely cost of an API call\n",
    "\n",
    "I've created a simple Python library to assist with estimating the cost of an API call before making the call. The library can be installed with `pip install llm_cost_estimation`. It exports the following objects:\n",
    "\n",
    "- models: Contains essential details about various LLMs, including cost per prompt token, cost per completion token, model description, and maximum allowed context length (in tokens).\n",
    "- count_token: A utility function to count the tokens present in a specific prompt or chat history using a given model's encoding system.\n",
    "- estimate_costs: A utility function to provide cost estimates for API calls to specified LLMs, based on selected model and length of text prompt or average length of messages in chat history.\n",
    "\n",
    "# Fetching a summary of model costs\n",
    "\n",
    "To view the models object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3c7a1 th {\n",
       "  max-width: 85px;\n",
       "  word-break: break-all;\n",
       "}\n",
       "#T_3c7a1_row0_col0, #T_3c7a1_row0_col2, #T_3c7a1_row0_col3, #T_3c7a1_row0_col4, #T_3c7a1_row1_col0, #T_3c7a1_row1_col2, #T_3c7a1_row1_col3, #T_3c7a1_row1_col4, #T_3c7a1_row2_col0, #T_3c7a1_row2_col2, #T_3c7a1_row2_col3, #T_3c7a1_row2_col4, #T_3c7a1_row3_col0, #T_3c7a1_row3_col2, #T_3c7a1_row3_col3, #T_3c7a1_row3_col4, #T_3c7a1_row4_col0, #T_3c7a1_row4_col2, #T_3c7a1_row4_col3, #T_3c7a1_row4_col4 {\n",
       "  max-width: 80px;\n",
       "}\n",
       "#T_3c7a1_row0_col1, #T_3c7a1_row1_col1, #T_3c7a1_row2_col1, #T_3c7a1_row3_col1, #T_3c7a1_row4_col1 {\n",
       "  max-width: 80px;\n",
       "  max-width: 280px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3c7a1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_3c7a1_level0_col0\" class=\"col_heading level0 col0\" >completion_cost_per_token</th>\n",
       "      <th id=\"T_3c7a1_level0_col1\" class=\"col_heading level0 col1\" >description</th>\n",
       "      <th id=\"T_3c7a1_level0_col2\" class=\"col_heading level0 col2\" >max_tokens</th>\n",
       "      <th id=\"T_3c7a1_level0_col3\" class=\"col_heading level0 col3\" >name</th>\n",
       "      <th id=\"T_3c7a1_level0_col4\" class=\"col_heading level0 col4\" >prompt_cost_per_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_3c7a1_row0_col0\" class=\"data row0 col0\" >0.002 / 1000</td>\n",
       "      <td id=\"T_3c7a1_row0_col1\" class=\"data row0 col1\" >Most capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration.</td>\n",
       "      <td id=\"T_3c7a1_row0_col2\" class=\"data row0 col2\" >4096</td>\n",
       "      <td id=\"T_3c7a1_row0_col3\" class=\"data row0 col3\" >gpt-3.5-turbo</td>\n",
       "      <td id=\"T_3c7a1_row0_col4\" class=\"data row0 col4\" >0.002 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3c7a1_row1_col0\" class=\"data row1 col0\" >0.06 / 1000</td>\n",
       "      <td id=\"T_3c7a1_row1_col1\" class=\"data row1 col1\" >Same capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.</td>\n",
       "      <td id=\"T_3c7a1_row1_col2\" class=\"data row1 col2\" >32768</td>\n",
       "      <td id=\"T_3c7a1_row1_col3\" class=\"data row1 col3\" >gpt-4-32k</td>\n",
       "      <td id=\"T_3c7a1_row1_col4\" class=\"data row1 col4\" >0.12 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3c7a1_row2_col0\" class=\"data row2 col0\" >0.06 / 1000</td>\n",
       "      <td id=\"T_3c7a1_row2_col1\" class=\"data row2 col1\" >More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration.</td>\n",
       "      <td id=\"T_3c7a1_row2_col2\" class=\"data row2 col2\" >8192</td>\n",
       "      <td id=\"T_3c7a1_row2_col3\" class=\"data row2 col3\" >gpt-4</td>\n",
       "      <td id=\"T_3c7a1_row2_col4\" class=\"data row2 col4\" >0.03 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3c7a1_row3_col0\" class=\"data row3 col0\" >0.0004 / 1000</td>\n",
       "      <td id=\"T_3c7a1_row3_col1\" class=\"data row3 col1\" >Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.</td>\n",
       "      <td id=\"T_3c7a1_row3_col2\" class=\"data row3 col2\" >2049</td>\n",
       "      <td id=\"T_3c7a1_row3_col3\" class=\"data row3 col3\" >text-ada-001</td>\n",
       "      <td id=\"T_3c7a1_row3_col4\" class=\"data row3 col4\" >0.0004 / 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3c7a1_row4_col0\" class=\"data row4 col0\" >0.0005 / 1000</td>\n",
       "      <td id=\"T_3c7a1_row4_col1\" class=\"data row4 col1\" >Capable of straightforward tasks, very fast, and lower cost.</td>\n",
       "      <td id=\"T_3c7a1_row4_col2\" class=\"data row4 col2\" >2049</td>\n",
       "      <td id=\"T_3c7a1_row4_col3\" class=\"data row4 col3\" >text-babbage-001</td>\n",
       "      <td id=\"T_3c7a1_row4_col4\" class=\"data row4 col4\" >0.0005 / 1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x24a770b7b50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_cost_estimation import models\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "models_df = pd.DataFrame(models)\n",
    "\n",
    "# Display the DataFrame\n",
    "models_df[:5].style\\\n",
    "    .hide(axis=\"index\")\\\n",
    "    .set_properties(**{'max-width': '80px'})\\\n",
    "    .set_properties(subset=['description'], **{'max-width': '280px'})\\\n",
    "    .set_table_styles([dict(selector=\"th\",props=[('max-width', '85px'),('word-break', 'break-all')])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting tokens in a prompt and estimating completion tokens\n",
    "\n",
    "The `count_tokens` function counts tokens in a prompt and makes a crude estimate of completion tokens. The estimate of completion tokens is based on the simple heuristic that the completion is likely to be of similar length to either the prompt or previous messages in the conversation. This heuristic is fast, but it may not be all that accurate. I'd love to have somebody [contribute](https://github.com/chriscarrollsmith/llm-cost-estimator) an optional alterantive estimation method, such as using a real-world measured prompt length to completion length ratio, or asking a very low-cost model like text-ada-001 guess how long the completion might be.\n",
    "\n",
    "To use the `count_tokens` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt: 6\n",
      "Estimated number of tokens in the completion: 6\n"
     ]
    }
   ],
   "source": [
    "from llm_cost_estimation import count_tokens\n",
    "\n",
    "text = \"Hello, how are you?\"\n",
    "model = \"gpt-4\"\n",
    "\n",
    "# Count tokens in the text\n",
    "prompt_tokens, estimated_completion_tokens = count_tokens(text, model)\n",
    "\n",
    "print(f\"Number of tokens in the prompt: {prompt_tokens}\")\n",
    "print(f\"Estimated number of tokens in the completion: {estimated_completion_tokens}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating cost of a prompt + completion\n",
    "\n",
    "The `estimate_cost` function tries to guess what the cost of a text completion will be, using `count_tokens` on its backend as a helper function.\n",
    "\n",
    "To use the `estimate_cost` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost of this completion: 0.0005399999999999999\n"
     ]
    }
   ],
   "source": [
    "from llm_cost_estimation import estimate_cost\n",
    "\n",
    "prompt = \"Hello, how are you?\"\n",
    "model = \"gpt-4\"\n",
    "\n",
    "# Estimate the cost for the completion\n",
    "estimated_cost = estimate_cost(prompt, model)\n",
    "\n",
    "print(f\"Estimated cost of this completion: {estimated_cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
