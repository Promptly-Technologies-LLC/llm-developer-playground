{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost tracking with langchain\n",
    "\n",
    "At this point, I think everyone should be using langchain for interacting with LLMs. This library has an incredibly powerful suite of tools. One task it makes very easy is tracking your API token usage and cost. Langchain accomplishes this through \"callbacks\" (a.k.a. \"context managers\"), its logging tools. Most langchain callbacks are only for event logging, but there is one special callback handler called `OpenAICallbackHandler` that does cost tracking.\n",
    "\n",
    "Note that this handler tracks cost cumulatively. For example, if you define the callback handler (`handler = OpenAICallbackHandler()`) you and supply it as an argument in two sequential calls to a model (`llm = OpenAI(callbacks=[handler])`), it will store the summed cost for both API requests rather than the cost for each individual request. \n",
    "\n",
    "Also note the following:\n",
    "\n",
    "- The `callbacks` argument takes a list, so you must always wrap your handler(s) in brackets.\n",
    "- The handler will be aliased as a list item in the model's `callbacks` property, so you can access it either by name or via that property.\n",
    "- The property is an alias, not a copy, which means that calls to a second model using the same handler will affect `callbacks` property of the first model.\n",
    "- To track two models separately, you can initialize a separate handler object for each model (`llm = OpenAI(callbacks=[OpenAICallbackHandler()])`).\n",
    "- `handler.total_tokens` stores the token count of prompt + completion as an int, while `handler.total_cost` stores the cost as a float. \n",
    "- The `prompt_tokens` and `completion_tokens` properties track prompt and completion token usage separately, since some OpenAI models charge different per-token rates for prompt and completion tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why did the chicken cross the road?\n",
      "\n",
      "To get to the other side!\n",
      "Tokens Used: 42\n",
      "\tPrompt Tokens: 4\n",
      "\tCompletion Tokens: 38\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00084\n",
      "Total cost of first prompt + completion: 0.00084\n",
      "\n",
      "\n",
      "Why did the chicken cross the road?\n",
      "\n",
      "To get to the other side!\n",
      "[Tokens Used: 84\n",
      "\tPrompt Tokens: 8\n",
      "\tCompletion Tokens: 76\n",
      "Successful Requests: 2\n",
      "Total Cost (USD): $0.00168]\n",
      "Total cost of first prompt + completion: 0.00168\n",
      "\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything.\n",
      "Total cost of first prompt + completion: 0.00248\n",
      "Total cost of first prompt + completion: 0.00248\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import OpenAICallbackHandler\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "handler = OpenAICallbackHandler()\n",
    "llm = OpenAI(model_name=\"text-davinci-002\",\n",
    "             n=2,\n",
    "             best_of=2,\n",
    "             temperature=0.95,\n",
    "             callbacks=[handler])\n",
    "\n",
    "# Track token usage over multiple API calls\n",
    "result = llm(\"Tell me a joke\")\n",
    "print(result)\n",
    "print(handler)\n",
    "print(\"Total cost of first prompt + completion: \" + str(handler.total_cost))\n",
    "result2 = llm(\"Tell me a joke\")\n",
    "print(result2)\n",
    "print(llm.callbacks)\n",
    "print(\"Total cost of first prompt + completion: \" + str(llm.callbacks[0].total_cost))\n",
    "\n",
    "llm2 = OpenAI(model_name=\"text-davinci-002\",\n",
    "             n=2,\n",
    "             best_of=2,\n",
    "             temperature=0.95,\n",
    "             callbacks=[handler])\n",
    "\n",
    "result3 = llm2(\"Tell me a joke\")\n",
    "print(result3)\n",
    "print(\"Total cost of first prompt + completion: \" + str(handler.total_cost))\n",
    "print(\"Total cost of first prompt + completion: \" + str(llm.callbacks[0].total_cost))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative syntax, you can use `with get_openai_callback() as cb`, which seems to be the preferred syntax in the langchain documentation. Note that this will scope your cost tracking much more locally, as it's effectively creating a handler object that applies only to the code you're wrapping, and then is removed from memory after this code runs. I find this approach to be of limited usefulness, but it may be appropriate for some use cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using langchain to make an API call, you can get the cost from the callback\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    # Track token usage over multiple API calls\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    result2 = llm(\"Tell me a joke\")\n",
    "\n",
    "    # Save cost to a variable of type float\n",
    "    print(cb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that instead of passing a callback handler to a model object, you can instead pass it to a chain (`chain = LLMChain(callbacks=[handler])`) or a call to a chain (`chain.call(inputs, callbacks=[handler])`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost tracking with OpenAI library\n",
    "\n",
    "If you're using the base `openai` library rather than `langchain`, you can get token usage, but not cost, directly from an API response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"test.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1684848788,\n",
      "  \"id\": \"chatcmpl-7JMQOncp5eXAWDijGoGrjBD8XVBXa\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 2,\n",
      "    \"prompt_tokens\": 11,\n",
      "    \"total_tokens\": 13\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[{\"role\": \"system\", \"content\": \"this is a\"}],\n",
    "        max_tokens=2,\n",
    "        temperature=0)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as you know the per-token cost of the model you're using, cost can be calculated from token usage. Thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of last API call: $0.000026\n"
     ]
    }
   ],
   "source": [
    "# Calculate token cost from token usage\n",
    "prompt_cost_per_token = 0.002 / 1000\n",
    "completion_cost_per_token = 0.002 / 1000\n",
    "cost_of_last_api_call = (\n",
    "        response[\"usage\"][\"prompt_tokens\"]*prompt_cost_per_token + \n",
    "        response[\"usage\"][\"completion_tokens\"]*completion_cost_per_token\n",
    "    )\n",
    "print(\"Cost of last API call: $\"+f'{cost_of_last_api_call:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
