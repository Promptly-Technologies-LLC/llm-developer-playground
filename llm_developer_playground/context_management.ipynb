{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Front-loading context for long-form content generation with OpenAI \"Turbo\" models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "OpenAI models have both context limits (various) and output limits (generally 4096 tokens). When working with potentially long inputs and outputs, we need to be able to manage both types of limits. \n",
    "\n",
    "The standard paradigm for long-form content editing with OpenAI's ChatCompletion API is to chunk the text into 4096-token segments, and then to edit them one at a time, using a rolling context window to provide some continuity between segments. This works fairly well for most purposes, but the advent of very long context windows in some models (up to 128,000 tokens) opens up the theoretical possibility of providing the model the entire document to be edited, up front, so that it can work with the entire context at once. This might, for instance, help the model recognize what kind of document it is working with, and accordingly tune its interpretation of text to be edited, translated, or otherwise operated upon.\n",
    "\n",
    "In this workbook, I explore this possibility, showing my work as I go. It was a surprisingly frustrating process, but I gained some valuable insights into how the OpenAI ChatCompletions API works, what the limitations of OpenAI's models are, and how to work around some common problems. I found that the GPT-4.5-Turbo model is actually extremely bad at tracking its progress through a long document over multiple text generation turns, and it also frequently stops prematurely.\n",
    "\n",
    "Prompt engineering can help. I can confirm, for instance, that offering to tip the model $20 for complete output helps mitigate the premature stopping problem. However, prompt engineering increases both token costs and potential failure points, the more likely the model will get confused and fail. I've tried to show my work in this notebook, but there was a lot of trial and error with prompts that I don't show here. (If you just want the final, working code, skip to the \"Putting it all together\" section at the end.)\n",
    "\n",
    "In general, I found that the better approach was to keep the prompt simple and the AI task relatively straightforward, and build lots of code around the inputs and outputs to manage the complexity. For instance, I had reasonably good success with numbering the paragraphs so the AI could track its progress through the document, though I still had to carefully explain how.\n",
    "\n",
    "All in all, I concluded that GPT-4.5-Turbo's 128,000 context length is still very limited for long-form content editing, and it's probably better, if possible, to stick with iteratively feeding the model shorter chunks. However, with *lots* of work, it *is* possible to make use of the long context window for multi-turn editing of front-loaded context, as I demonstrate here.\n",
    "\n",
    "## Counting/estimating input tokens\n",
    "\n",
    "For this exercise, we'll use a very long classic literary work obtained from Project Gutenberg-- Moby Dick, by Herman Melville-- to simulate a very long context input (>128,000 tokens). And we will choose a prompt that requires output of equal length.\n",
    "\n",
    "OpenAI offers this heuristic for estimating token length: about 1 token per 4 characters of English text. We can check this estimate against an actual token count generated with the `tiktoken` Python library to confirm that this is in the right ballpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated token count: 297792\n",
      "Actual token count: 281265\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import ChatCompletion\n",
    "from openai import BadRequestError\n",
    "import tiktoken\n",
    "import warnings\n",
    "\n",
    "model_cost = {\n",
    "    \"gpt-3.5-turbo\": (0.0005/1000, 0.0015/1000),\n",
    "    \"gpt-4\": (0.03/1000, 0.06/1000),\n",
    "    \"gpt-4-32k\": (0.06/1000, 0.12/1000),\n",
    "    \"gpt-4-turbo-preview\": (0.01/1000, 0.03/1000)\n",
    "}\n",
    "\n",
    "def estimate_context_length(context: str) -> int:\n",
    "    # Count characters in the input\n",
    "    prompt_length = len(context)\n",
    "\n",
    "    # Assume ~4 characters per token\n",
    "    prompt_token_count = prompt_length/4\n",
    "\n",
    "    return int(prompt_token_count)\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "with open(\"sample_input.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    long_text = file.read()\n",
    "\n",
    "print(\"Estimated token count: \" + str(estimate_context_length(long_text)))\n",
    "\n",
    "# Count the tokens in long_text\n",
    "print(\"Actual token count: \" + str(count_tokens(long_text, \"gpt-3.5-turbo\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling errors related to violations of context length\n",
    "\n",
    "We have an input of approximately 300,000 tokens. The estimate is a little above the actual token count, which is good because it means this estimation method will tend to err conservatively on the side of selecting a longer context model. If we naively submit this input to the OpenAI ChatCompletion API, we surprisingly get a `RateLimitError`, because our organization is only allowed to transmit 160,000 tokens per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.RateLimitError'>\n",
      "Error code: 429 - {'error': {'message': 'Request too large for gpt-3.5-turbo-0613 in organization org-EitsfepwgP6MWPVn6teUfr7t on tokens per min (TPM): Limit 160000, Requested 301753. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "system_prompt = \"The user will provide a portion of the text of Herman Melville's \" + \\\n",
    "\"classic public domain novel Moby Dick. Your task is to translate the text into \" + \\\n",
    "\"modern American vernacular. You should try to preserve the meaning and setting \" + \\\n",
    "\"of the text, while modernizing its voice. You are only responsible to translate \" + \\\n",
    "\"the portion of the text in the user's prompt, but you must provide a full, \" + \\\n",
    "\"unabbreviated translation of this text.\"\n",
    "\n",
    "message_list = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": long_text}\n",
    "            ]\n",
    "\n",
    "try:\n",
    "    response: ChatCompletion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=message_list,\n",
    "            max_tokens=4096,\n",
    "            temperature=0\n",
    "        )\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(type(e))\n",
    "    print(e.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we submit an input below the rate limit, but longer than the context window of the model (4097 tokens in the case of `gpt-3.5-turbo-0613`)? Let's submit about 10,000 tokens to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.BadRequestError'>\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 9584 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "medium_text = long_text[:10000*4]\n",
    "\n",
    "message_list = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": medium_text},\n",
    "            ]\n",
    "\n",
    "try:\n",
    "    response: ChatCompletion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=message_list,\n",
    "            max_tokens=4096,\n",
    "            temperature=0\n",
    "        )\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(type(e))\n",
    "    print(e.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we get a `BadRequestError`, and the `message` attribute contains the string `context_length_exceeded`. We could use string matching for error handling.\n",
    "\n",
    "The same problem arises if our input is below the context limit, but we supply a `max_tokens` parameter that, added to the context length, would cumulatively result in exceeding the context limit. For example, we can submit 3,000 tokens of input and specify a maximum of 4096 tokens of output. When we try this, we get the same `BadRequestError` and `context_length_exceeded` code as we got above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.BadRequestError'>\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, you requested 6973 tokens (2877 in the messages, 4096 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "short_text = long_text[:3000*4]\n",
    "\n",
    "message_list = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": short_text}\n",
    "            ]\n",
    "\n",
    "try:\n",
    "    response: ChatCompletion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=message_list,\n",
    "            max_tokens=4096,\n",
    "            temperature=0\n",
    "        )\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(type(e))\n",
    "    print(e.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remove the max_tokens parameter, we can successfully submit the 3,000 token input and receive an output. However, the output will be truncated at 4096 (cumulative) tokens, with `response.choices[0].finish_reason == 'length'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8vY9l3kUDBY98T5oizwxjQvJ7z41m', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"CHAPTER 1. Loomings.\\n\\nCall me Ishmael. A few years ago—never mind exactly how long—I was broke and had nothing better to do on land, so I decided to go out to sea and see what the ocean had to offer. It's my way of getting rid of my bad mood and clearing my head. Whenever I start feeling down and depressed, or when it's a gloomy November in my soul, and I find myself staring at coffins and following funerals, or when my anxieties start overwhelming me to the point where I have to fight the urge to go out and knock people's hats off, that's when I know it's time to hit the sea. It's my way of dealing with things. Cato throws himself on his sword to find peace, but I prefer to quietly board a ship. It's not that surprising, really. If people knew, most of them would feel the same way about the ocean as I do, to some extent.\\n\\nNow, let's talk about Manhattan, that island city surrounded by docks like coral reefs. The streets all lead to the water. The southernmost point is the battery, where the waves wash up against a noble pier, cooled by breezes that were out of sight of land just a few hours ago. Look at all the people there, gazing out at the water.\\n\\nTake a walk around the city on a lazy Sunday afternoon. Start from Corlears Hook and head north through Coenties Slip and Whitehall. What do you see? People standing silently all around the town, lost in their thoughts about the ocean. Some leaning against the docks, some sitting on the pier, some looking out from the ships from China, and some high up in the rigging, trying to get a better view of the sea. But these are all landlubbers, stuck in their weekday routines, tied to their jobs and desks. How did they end up here?\\n\\nBut look! Here come more crowds, heading straight for the water, as if they're about to dive in. Strange! They won't be satisfied until they're as close to the water as possible without falling in. And there they stand, for miles and miles. They come from all over the city, from every direction. Does the magnetic pull of the compass needles on those ships attract them?\\n\\nLet's say you're in the countryside, in a highland with lakes. Take any path you want, and most likely it will lead you down to a valley, where you'll find a pool in a stream. There's something magical about it. Even the most absent-minded person, lost in their thoughts, will lead you to water if there's any around. If you ever find yourself thirsty in the middle of the American desert, try this experiment, if you happen to have a philosopher with you. Yes, as everyone knows, meditation and water go hand in hand.\\n\\nNow, imagine an artist. He wants to paint the dreamiest, most peaceful, enchanting landscape in the Saco Valley. What's the most important element he uses? There are the trees, with hollow trunks as if there's a hermit and a crucifix inside. There's the meadow, where the cattle sleep, and the sleepy smoke rising from a cottage. A winding path leads into the distant woods, reaching the blue mountains. But even though the picture is beautiful, it's all in vain unless the shepherd's eye is fixed on the magic stream in front of him. Go visit the Prairies in June, where you'll wade through fields of Tiger-lilies for miles and miles. What's missing? Water. There's not a drop of water there! If Niagara Falls were just a sand dune, would you travel a thousand miles to see it? Why did the poor poet from Tennessee, when he suddenly received some money, have to decide between buying a coat he desperately needed or going on a trip to Rockaway Beach? Why is every healthy, adventurous boy at some point crazy about going to sea? Why did you feel a mystical vibration on your first voyage as a passenger, when you were told that you and the ship were out of sight of land? Why did the ancient Persians consider the sea holy? Why did the Greeks worship it as a separate deity, a brother of Zeus? There's definitely a deeper meaning to all of this. And even deeper is the meaning behind the story of Narcissus, who couldn't grasp the image he saw in the fountain and ended up drowning in it. But that same image, we see it in every river and ocean. It's the image of the ungraspable mystery of life, and that's the key to it all.\\n\\nSo, when I say that I go to sea whenever my vision gets blurry and I become too aware of my breathing, I don't mean that I go as a passenger. To be a passenger, you need money, and money is useless if you don't have anything in it. Plus, passengers get seasick, they argue, they can't sleep at night, and they don't really enjoy themselves most of the time. No, I never go as a passenger. And even though I have some experience as a sailor, I don't go as a Commodore, a Captain, or a Cook. I leave those prestigious positions to those who want them. Personally, I hate all the respectable and honorable responsibilities and challenges that come with them. Taking care of myself is already enough, I don't need to worry about ships and all that. And as for being a cook, even though there's some glory in it, being an officer on a ship, I never really liked cooking fowls. Although, once they're broiled, buttered, salted, and peppered just right, I can speak very respectfully, even reverently, about a broiled fowl. The ancient Egyptians idolized broiled ibis and roasted river horse, which is why you can find\", role='assistant', function_call=None, tool_calls=None))], created=1708726685, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1221, prompt_tokens=2877, total_tokens=4098))\n"
     ]
    }
   ],
   "source": [
    "message_list = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": short_text}\n",
    "    ]\n",
    "\n",
    "response: ChatCompletion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=message_list,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `finish_reason==length` to fall back to a longer context model\n",
    "\n",
    "If we encounter this scenario where `finish_reason==length`, we can add the response to the `messages` list, instruct the model to pick up where it left off, and substitute a longer context model (such as gpt-3.5-turbo-0125, with 16k context length) for the `model` parameter. (One of the fun things about working with LLMs through an API is that we can swap different models in and out of the same chat.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8vYAf3KJ4VwqFBjqYkcqjnTrWVgDQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='mummified versions of those animals in the pyramids.\\n\\nNo, when I go to sea, I go as a simple sailor, working hard on the deck, from the front to the top. Sure, they boss me around a bit, making me jump from one place to another like a grasshopper in a field. At first, it\\'s not the most pleasant experience. It challenges my sense of honor, especially coming from a respected family like the Van Rensselaers or Randolphs. And especially if, just before getting my hands dirty with tar, I was a teacher making the big kids fear me. The transition from a teacher to a sailor is a tough one, and it takes a lot of mental strength to handle it. But eventually, you get used to it.\\n\\nSo what if an old sea captain tells me to grab a broom and sweep the decks? How much of an insult is that, really, when you think about it in the grand scheme of things? Do you think the archangel Gabriel would think any less of me because I obey that old captain promptly and respectfully? We\\'re all slaves in some way or another. So, even though those old sea captains might bark orders at me, and even though they might push and shove me around, I find comfort in knowing that it\\'s all part of the deal. Everyone else gets their fair share of physical or mental challenges, too. So, we all take our turn, and we should all support each other.\\n\\nAnd another reason I go to sea as a sailor is because they actually pay me for my work, unlike passengers who have to pay for the privilege. There\\'s a big difference between paying and getting paid. Paying can be uncomfortable, like the punishment Adam and Eve faced in the Garden of Eden. But getting paid—what a feeling! The way a person receives money with grace is truly remarkable, considering how we often see money as the root of all evil and believe that rich people can\\'t enter heaven. It\\'s funny how willingly we accept our damnation!\\n\\nLastly, I go to sea as a sailor for the physical activity and fresh air on the deck. Just like in life, we often face challenges (headwinds) more than we get help (tailwinds), so the Commodore on the fancy deck actually gets his air from the sailors working hard on the front deck. He thinks he\\'s breathing it first, but that\\'s not the case. In many ways, the common folks lead their leaders without them even realizing it. But why, after smelling the sea as a merchant sailor, did I suddenly decide to go on a whaling voyage? The mysterious forces that watch over me and guide me in strange ways can answer that better than anyone else. It must have been part of a bigger plan, a brief interlude in the grand scheme of things. Maybe it went something like this:\\n\\n\"Grand Election for the Presidency of the United States. \"WHALING VOYAGE BY ONE ISHMAEL. \"BLOODY BATTLE IN AFGHANISTAN.\"\\n\\nI can\\'t explain why the Fates chose me for this humble role on a whaling voyage, while others got grand parts in high dramas or easy roles in comedies. But looking back, I can see how various motives and disguises led me to take on this role, making me believe it was my own choice based on free will and judgment.\\n\\nOne of the main reasons was the fascination with the great whale itself. Such a mysterious and monstrous creature sparked my curiosity. The wild and distant seas where it roamed, the nameless dangers it posed, along with the wonders of Patagonia, all played a part in my decision. For others, these things might not have been appealing, but for me, I have an insatiable curiosity for the unknown. I love exploring forbidden territories and landing on unfamiliar shores. While I appreciate the good things in life, I\\'m also quick to recognize the horrors and can still find a way to connect with them, if given the chance.\\n\\nBecause of all these reasons, the whaling voyage was a welcome opportunity for me. It opened the floodgates to a world of wonders, and in the wi', role='assistant', function_call=None, tool_calls=None))], created=1708726741, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_86156a94a0', usage=CompletionUsage(completion_tokens=857, prompt_tokens=4102, total_tokens=4959))\n"
     ]
    }
   ],
   "source": [
    "message_list.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "\n",
    "response: ChatCompletion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    messages=message_list,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works as expected, returning a total of about 6500 tokens with `finish_reason == 'stop'`. We can simply concatenate the content from messages with role 'assistant' to get the full output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER 1. Loomings.\n",
      "\n",
      "Call me Ishmael. A few years ago—never mind exactly how long—I was broke and had nothing better to do on land, so I decided to go out to sea and see what the ocean had to offer. It's my way of getting rid of my bad mood and clearing my head. Whenever I start feeling down and depressed, or when it's a gloomy November in my soul, and I find myself staring at coffins and following funerals, or when my anxieties start overwhelming me to the point where I have to fight the urge to go out and knock people's hats off, that's when I know it's time to hit the sea. It's my way of dealing with things. Cato throws himself on his sword to find peace, but I prefer to quietly board a ship. It's not that surprising, really. If people knew, most of them would feel the same way about the ocean as I do, to some extent.\n",
      "\n",
      "Now, let's talk about Manhattan, that island city surrounded by docks like coral reefs. The streets all lead to the water. The southernmost point is the battery, where the waves wash up against a noble pier, cooled by breezes that were out of sight of land just a few hours ago. Look at all the people there, gazing out at the water.\n",
      "\n",
      "Take a walk around the city on a lazy Sunday afternoon. Start from Corlears Hook and head north through Coenties Slip and Whitehall. What do you see? People standing silently all around the town, lost in their thoughts about the ocean. Some leaning against the docks, some sitting on the pier, some looking out from the ships from China, and some high up in the rigging, trying to get a better view of the sea. But these are all landlubbers, stuck in their weekday routines, tied to their jobs and desks. How did they end up here?\n",
      "\n",
      "But look! Here come more crowds, heading straight for the water, as if they're about to dive in. Strange! They won't be satisfied until they're as close to the water as possible without falling in. And there they stand, for miles and miles. They come from all over the city, from every direction. Does the magnetic pull of the compass needles on those ships attract them?\n",
      "\n",
      "Let's say you're in the countryside, in a highland with lakes. Take any path you want, and most likely it will lead you down to a valley, where you'll find a pool in a stream. There's something magical about it. Even the most absent-minded person, lost in their thoughts, will lead you to water if there's any around. If you ever find yourself thirsty in the middle of the American desert, try this experiment, if you happen to have a philosopher with you. Yes, as everyone knows, meditation and water go hand in hand.\n",
      "\n",
      "Now, imagine an artist. He wants to paint the dreamiest, most peaceful, enchanting landscape in the Saco Valley. What's the most important element he uses? There are the trees, with hollow trunks as if there's a hermit and a crucifix inside. There's the meadow, where the cattle sleep, and the sleepy smoke rising from a cottage. A winding path leads into the distant woods, reaching the blue mountains. But even though the picture is beautiful, it's all in vain unless the shepherd's eye is fixed on the magic stream in front of him. Go visit the Prairies in June, where you'll wade through fields of Tiger-lilies for miles and miles. What's missing? Water. There's not a drop of water there! If Niagara Falls were just a sand dune, would you travel a thousand miles to see it? Why did the poor poet from Tennessee, when he suddenly received some money, have to decide between buying a coat he desperately needed or going on a trip to Rockaway Beach? Why is every healthy, adventurous boy at some point crazy about going to sea? Why did you feel a mystical vibration on your first voyage as a passenger, when you were told that you and the ship were out of sight of land? Why did the ancient Persians consider the sea holy? Why did the Greeks worship it as a separate deity, a brother of Zeus? There's definitely a deeper meaning to all of this. And even deeper is the meaning behind the story of Narcissus, who couldn't grasp the image he saw in the fountain and ended up drowning in it. But that same image, we see it in every river and ocean. It's the image of the ungraspable mystery of life, and that's the key to it all.\n",
      "\n",
      "So, when I say that I go to sea whenever my vision gets blurry and I become too aware of my breathing, I don't mean that I go as a passenger. To be a passenger, you need money, and money is useless if you don't have anything in it. Plus, passengers get seasick, they argue, they can't sleep at night, and they don't really enjoy themselves most of the time. No, I never go as a passenger. And even though I have some experience as a sailor, I don't go as a Commodore, a Captain, or a Cook. I leave those prestigious positions to those who want them. Personally, I hate all the respectable and honorable responsibilities and challenges that come with them. Taking care of myself is already enough, I don't need to worry about ships and all that. And as for being a cook, even though there's some glory in it, being an officer on a ship, I never really liked cooking fowls. Although, once they're broiled, buttered, salted, and peppered just right, I can speak very respectfully, even reverently, about a broiled fowl. The ancient Egyptians idolized broiled ibis and roasted river horse, which is why you can find mummified versions of those animals in the pyramids.\n",
      "\n",
      "No, when I go to sea, I go as a simple sailor, working hard on the deck, from the front to the top. Sure, they boss me around a bit, making me jump from one place to another like a grasshopper in a field. At first, it's not the most pleasant experience. It challenges my sense of honor, especially coming from a respected family like the Van Rensselaers or Randolphs. And especially if, just before getting my hands dirty with tar, I was a teacher making the big kids fear me. The transition from a teacher to a sailor is a tough one, and it takes a lot of mental strength to handle it. But eventually, you get used to it.\n",
      "\n",
      "So what if an old sea captain tells me to grab a broom and sweep the decks? How much of an insult is that, really, when you think about it in the grand scheme of things? Do you think the archangel Gabriel would think any less of me because I obey that old captain promptly and respectfully? We're all slaves in some way or another. So, even though those old sea captains might bark orders at me, and even though they might push and shove me around, I find comfort in knowing that it's all part of the deal. Everyone else gets their fair share of physical or mental challenges, too. So, we all take our turn, and we should all support each other.\n",
      "\n",
      "And another reason I go to sea as a sailor is because they actually pay me for my work, unlike passengers who have to pay for the privilege. There's a big difference between paying and getting paid. Paying can be uncomfortable, like the punishment Adam and Eve faced in the Garden of Eden. But getting paid—what a feeling! The way a person receives money with grace is truly remarkable, considering how we often see money as the root of all evil and believe that rich people can't enter heaven. It's funny how willingly we accept our damnation!\n",
      "\n",
      "Lastly, I go to sea as a sailor for the physical activity and fresh air on the deck. Just like in life, we often face challenges (headwinds) more than we get help (tailwinds), so the Commodore on the fancy deck actually gets his air from the sailors working hard on the front deck. He thinks he's breathing it first, but that's not the case. In many ways, the common folks lead their leaders without them even realizing it. But why, after smelling the sea as a merchant sailor, did I suddenly decide to go on a whaling voyage? The mysterious forces that watch over me and guide me in strange ways can answer that better than anyone else. It must have been part of a bigger plan, a brief interlude in the grand scheme of things. Maybe it went something like this:\n",
      "\n",
      "\"Grand Election for the Presidency of the United States. \"WHALING VOYAGE BY ONE ISHMAEL. \"BLOODY BATTLE IN AFGHANISTAN.\"\n",
      "\n",
      "I can't explain why the Fates chose me for this humble role on a whaling voyage, while others got grand parts in high dramas or easy roles in comedies. But looking back, I can see how various motives and disguises led me to take on this role, making me believe it was my own choice based on free will and judgment.\n",
      "\n",
      "One of the main reasons was the fascination with the great whale itself. Such a mysterious and monstrous creature sparked my curiosity. The wild and distant seas where it roamed, the nameless dangers it posed, along with the wonders of Patagonia, all played a part in my decision. For others, these things might not have been appealing, but for me, I have an insatiable curiosity for the unknown. I love exploring forbidden territories and landing on unfamiliar shores. While I appreciate the good things in life, I'm also quick to recognize the horrors and can still find a way to connect with them, if given the chance.\n",
      "\n",
      "Because of all these reasons, the whaling voyage was a welcome opportunity for me. It opened the floodgates to a world of wonders, and in the wi\n"
     ]
    }
   ],
   "source": [
    "message_list.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "\n",
    "def get_full_output(message_list):\n",
    "    # Join content with a space, ensuring 'content' key exists and is not empty\n",
    "    return \" \".join([message[\"content\"] for message in message_list if message.get(\"role\") == \"assistant\" and message.get(\"content\", \"\")])\n",
    "\n",
    "full_output = get_full_output(message_list)\n",
    "\n",
    "print(full_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context length is less of an issue with state-of-the-art models than it used to be, because the current flagship GPT-3.5-Turbo model has a context length of 16,385 tokens. That comes out to about 65,000 characters of English text, or almost 11,000 words. That's long enough to accommodate your typical academic paper, though it falls short of most novellas and books. Thus, context management remains important for edge cases. And even for shorter inputs, we still need to manage output length because of the 4096 token output limit.\n",
    "\n",
    "To handle the output limit, there's basically only one viable approach: check for the `length` finish reason, and if it's present, add the response to the `messages` list, instruct the model to pick up where it left off, and re-prompt. Unfortunately this is fairly high latency and leads to some redundancy in token usage, but it is what it is.\n",
    "\n",
    "For managing context length, meanwhile, we have a couple options: we can either pre-estimate the necessary context length and select an appropriate model, or we can start with the smallest available model and fall back to a larger one whenever we run up against (or estimate that we will run up against) the context limit. The latter approach is less expensive in terms of token cost and latency, but it also risks poorer output, even from the more powerful fallback model, because the smaller model's output acts as part of the prompt for, and sets the pattern for, the larger model's output. (In fact, if we cared about maximizing output quality rather than minimizing cost, we might start with `gpt-4` or `gpt-4-32k`—with respective context lengths of 8,192 and 32,768 tokens—rather than `gpt-3.5-turbo`. However, in that case we might want to be a little stingier with our input tokens on the first pass instead of dumping the whole text into context!)\n",
    "\n",
    "The following code demonstrates the first approach, switching from a smaller to a larger model as necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message list length: 2, Total tokens used: 0\n",
      "Response appended to message list.\n",
      "Tokens used:\n",
      "10663\n",
      "Full output:\n",
      "Call me Ishmael. So, a while back—never mind exactly how long—when I was pretty much broke, with no cash in my pocket and nothing exciting happening on land, I figured I'd hit the seas for a bit and check out the watery side of the world. It's my way of shaking off the blues and getting my blood pumping. Whenever I start feeling all serious and gloomy; when my soul feels like a damp, drizzly November day; when I catch myself staring at coffin shops and following funerals; and especially when my bad moods start making me want to knock people's hats off in the street, then I know it's time to head to sea. It's my way of dealing with things. Cato might throw himself on his sword with a flourish, but I prefer to hop on a ship quietly. It's not that surprising. If people knew, most of them probably feel the same way about the ocean at some point in their lives.\n",
      "\n",
      "Now, picture this: your isolated city of Manhattan, surrounded by docks like coral reefs around an island—commerce crashing against it like waves. Streets leading you straight to the water on both sides. Down at the very tip is the Battery, where the noble pier is washed by waves and cooled by breezes that just a few hours ago were far out at sea. Look at all the people gazing at the water.\n",
      "\n",
      "Take a walk around the city on a dreamy Sunday afternoon. Go from Corlears Hook to Coenties Slip, and then up north through Whitehall. What do you see? People standing like silent guards all around the town, lost in thoughts of the ocean. Some leaning against the posts, some sitting on the pier-heads, some peering over the ship bulwarks from China; some high up in the rigging, trying to get a better view of the sea. But these are all landlubbers; during the week, they're stuck in buildings—tied to counters, nailed to benches, chained to desks. How did they end up here? Where did the green fields go? What are they doing here?\n",
      "\n",
      "But look! Here come more crowds, heading straight for the water, looking like they're about to dive in. Strange! Nothing satisfies them but getting as close to the water as possible without falling in. And there they stand—miles of them—endless rows. Inlanders, all of them, coming from alleys, streets, and avenues—north, east, south, and west. Yet here they all gather. Tell me, does some magnetic force from the compass needles of those ships draw them there?\n",
      "\n",
      "Let's try this again. Imagine you're in the countryside, in some high land of lakes. Choose any path you like, and chances are it will lead you down to a valley, leaving you by a stream or pool. There's something magical about it. Even the most absent-minded person, lost in thoughts, if you set him walking, he'll lead you straight to water, if there's any around. If you ever find yourself thirsty in the vast American desert, try this experiment, if you happen to have a philosopher in your group. Yes, as everyone knows, meditation and water are forever linked.\n",
      "\n",
      "But here's an artist. He wants to paint you the dreamiest, most peaceful, enchanting scene in the Saco Valley. What's his main focus? There are the trees, each with a hollow trunk, as if a hermit and a crucifix were inside; there's the meadow, the cattle, and the smoke rising from a distant cottage. A winding path leads deep into the woods, reaching the mountain spurs bathed in their blue hues. But even with all this beauty, it would be meaningless without the shepherd's gaze fixed on the magic stream before him. Go visit the Prairies in June, where you wade through fields of Tiger-lilies for miles—what's missing? Water—there's not a drop to be found! If Niagara were just a sand waterfall, would you travel a thousand miles to see it? Why did the poet from Tennessee, upon receiving two handfuls of silver, debate whether to buy a coat he desperately needed or take a trip to Rockaway Beach? Why does every healthy, adventurous boy at some point dream of going to sea? Why did you feel a mysterious thrill on your first voyage when you were told you were out of sight of land? Why did the ancient Persians revere the sea? Why did the Greeks give it a godly status, a sibling of Jove? Surely, there's a deeper meaning to all this. And even deeper is the story of Narcissus, who, unable to grasp the image he saw in the fountain, dove in and drowned. But that same image, we see it in every river and ocean. It's the ungraspable essence of life; and that's the key to it all.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"The user will provide a portion of the text of Herman Melville's \" + \\\n",
    "\"classic public domain novel Moby Dick. Your task is to translate the text into \" + \\\n",
    "\"modern American vernacular. You should try to preserve the meaning and setting \" + \\\n",
    "\"of the text, while modernizing its voice. You are only responsible to translate \" + \\\n",
    "\"the portion of the text in the user's prompt, but you must provide a full, \" + \\\n",
    "\"unabbreviated translation of this text. Your response will be parsed \" + \\\n",
    "\"by software and not by a human, and you will only be prompted for a continuation \" + \\\n",
    "\"if your response is interrupted for reasons of length, so do not use a stop \" + \\\n",
    "\"token until you are completely done with the translation task. If your response \" + \\\n",
    "\"is interrupted, pick up in the next message exactly where you left off.\"\n",
    "\n",
    "# Define LLM parameters\n",
    "models = {\n",
    "        16384: \"gpt-3.5-turbo\",\n",
    "        128000: \"gpt-4-turbo-preview\"\n",
    "    }\n",
    "\n",
    "def select_model(message_list: list[dict], minimum_output_tokens: int = 4096) -> str:\n",
    "    # Validate minimum_output_tokens\n",
    "    if minimum_output_tokens < 1 or minimum_output_tokens > 4096:\n",
    "        raise ValueError(\"minimum_output_tokens must be between 1 and 4096.\")\n",
    "\n",
    "    # Estimate the context length including the minimum output tokens\n",
    "    input_tokens = estimate_context_length(str(message_list))\n",
    "    total_tokens = input_tokens + minimum_output_tokens\n",
    "\n",
    "    # Filter models that can handle the total_tokens and select the smallest one\n",
    "    suitable_models = [size for size in models.keys() if size >= total_tokens]\n",
    "\n",
    "    if not suitable_models:\n",
    "        # If no model supports the context length, raise an error\n",
    "        # (Note: This constraint *should* prevent RateLimitError and BadRequestError.)\n",
    "        raise ValueError(\"Context length exceeds the maximum supported token count.\")\n",
    "\n",
    "    selected_model_size = min(suitable_models)\n",
    "    return models[selected_model_size]\n",
    "\n",
    "\n",
    "def prompt_llm(\n",
    "            message_list: list[dict],\n",
    "            minimum_output_tokens: int = 4096,\n",
    "            total_tokens_used: int = 0\n",
    "        ) -> tuple[list[dict], int]:\n",
    "\n",
    "    print(f\"Message list length: {len(message_list)}, Total tokens used: {total_tokens_used}\")\n",
    "\n",
    "    # Select appropriate model based on prompt length and minimum_output_tokens\n",
    "    model = select_model(message_list, minimum_output_tokens)\n",
    "\n",
    "    try:\n",
    "        # Prompt the LLM with the current message_list\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=message_list,\n",
    "            temperature=0\n",
    "        )\n",
    "        # Update the message list with the response\n",
    "        message_list.append(\n",
    "                {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "            )\n",
    "        # Update total token usage\n",
    "        total_tokens_used += response.usage.total_tokens\n",
    "\n",
    "        print(\"Response appended to message list.\")\n",
    "\n",
    "        # If response was interrupted due to length, append response to message_list\n",
    "        # and call function recursively with updated message_list to prompt again\n",
    "        if response.choices[0].finish_reason == \"length\":\n",
    "            print(\"Response finish reason: length. Recursing with updated message list.\")\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                minimum_output_tokens,\n",
    "                total_tokens_used\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        # Try selecting model with more conservative assumptions before raising BadRequestError\n",
    "        if isinstance(e, BadRequestError) and minimum_output_tokens < 4096:\n",
    "            print(\"BadRequestError encountered. Retrying with minimum_output_tokens=4096.\")\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                4096,\n",
    "                total_tokens_used\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Exception encountered: {e}\")\n",
    "            raise e\n",
    "\n",
    "    return message_list, total_tokens_used\n",
    "\n",
    "message_list_prompt = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": medium_text}\n",
    "            ]\n",
    "\n",
    "message_list_result, tokens_used = prompt_llm(message_list_prompt, 4096)\n",
    "full_output = get_full_output(message_list_result)\n",
    "print(\"Tokens used:\")\n",
    "print(tokens_used)\n",
    "print(\"Full output:\")\n",
    "print(full_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we run into a problem when we try to use `finish_reason=='length'` as a condition for continuing the LLM's response. Apparently, the 'length' stopping reason only occurs when the model reaches the end of its context length. I cannot seem to get the model to overflow its 4096 output token limit and throw this signal, even if I explicitly instruct it to do so. Thus, we have to find a different heuristic for determining whether to continue generating text or stop.\n",
    "\n",
    "## Using a stopping signal to determine whether the model has completed the task\n",
    "\n",
    "Instead of using `finish_reason`, we can explicitly ask the model to tell us if it has completed the task. If it has, we can break the loop. If it hasn't, we can continue. The danger here is that the model might misrender our stopping signal, and we might therefore fail to detect it. We can mitigate this risk by using fuzzy string matching to detect the stopping signal, and by using a recursion depth limit as a last resort to prevent infinite loops (although our context length constraints should already prevent this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursion depth: 0, Message list length: 2, Total tokens used: 0\n",
      "Response appended to message list.\n",
      "Assistant response is not 'STOP'. Appending user prompt and recursing.\n",
      "Recursion depth: 1, Message list length: 4, Total tokens used: 10582\n",
      "Response appended to message list.\n",
      "Assistant response is 'STOP'. Returning message list.\n",
      "Tokens used:\n",
      "21201\n",
      "Full output:\n",
      "Call me Ishmael. So, a while back—don't worry about the exact time—when I was broke and bored on land, I decided to hit the seas for a change of scenery. It's my way of shaking off the blues and getting some fresh air. Whenever I start feeling down, or when the weather matches my mood, or when I catch myself lingering near funeral homes and following funerals, or especially when my worries start getting the best of me to the point where I have to really fight the urge to start knocking people's hats off in the street, that's when I know it's time to head out to sea. It's my way of clearing my head. Instead of resorting to violence, like some folks might with a gun or a sword, I opt for the sea. It's not that unusual. If people really understood, most folks, at some point, feel a similar pull towards the ocean like I do.\n",
      "\n",
      "Now picture this: Manhattan, surrounded by docks like islands are by coral reefs—commerce crashing around it like waves. Everywhere you look, the streets lead you to the water. Down at the very tip is the Battery, where the shore is kissed by waves and cooled by breezes that just a few hours ago were far out at sea. Look at all the people gazing out at the water.\n",
      "\n",
      "Take a walk around the city on a lazy Sunday afternoon. From Corlears Hook to Coenties Slip, and then up north through Whitehall. What do you see? People standing like silent guards all around the town, lost in thoughts of the ocean. Some leaning against the posts, some sitting on the pier, some peering over the sides of ships from China, some high up in the rigging trying to get a better view of the sea. But these are all landlubbers; during the week, they're stuck indoors, tied to counters, nailed to benches, glued to desks. How did they end up here? Where did the green fields go? What are they doing here?\n",
      "\n",
      "But look! Here come more crowds, heading straight for the water, as if they're about to dive in. Strange! Nothing satisfies them except getting as close to the water as possible without falling in. And there they stand—miles of them—endless rows. Inlanders from all over, coming from alleys, streets, and avenues—north, east, south, and west. Yet here they all gather. Tell me, is there some magnetic force drawing them to the compass needles of those ships?\n",
      "\n",
      "Let's try this again. Imagine you're in the countryside, in a land of lakes. Take any path you like, and chances are it will lead you down to a valley, by a pool in a stream. There's something magical about it. Even the most absent-minded person, lost in thought, if you stand them up and set them walking, they'll inevitably lead you to water, if there's any around. If you ever find yourself thirsty in the desert, try this out, if you happen to have a philosopher in your group. Yes, as everyone knows, meditation and water are forever linked.\n",
      "\n",
      "But here's an artist. He wants to paint you the dreamiest, most peaceful, enchanting landscape in the Saco Valley. What's his secret? Picture the trees, each with a hollow trunk, as if a hermit and a crucifix were inside; the meadow where his cattle graze, the smoke rising from a distant cottage. A winding path leads deep into the woods, reaching the mountains bathed in blue on the hillside. But even with all this beauty, it would be meaningless without the shepherd's gaze fixed on the magic stream before him. Visit the Prairies in June, wading through fields of Tiger-lilies for miles—what's missing? Water—there's not a drop to be found! If Niagara Falls were just a sand dune, would you travel a thousand miles to see it? Why did the poet from Tennessee, upon receiving silver, debate whether to buy a coat or take a trip to Rockaway Beach? Why does every healthy boy, full of life, at some point dream of going to sea? Why did you feel a mysterious thrill on your first voyage, knowing you were out of sight of land? Why did the ancient Persians revere the sea? Why did the Greeks give it a godly status, a sibling of Jove? Surely, there's a deeper meaning to all this. And even deeper is the story of Narcissus, who, unable to grasp the image he saw in the fountain, dove in and drowned. But that same image, we see it in every river and ocean. It's the ungraspable essence of life; that's the key to it all. STOP\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"The user will provide a portion of the text of Herman Melville's \" + \\\n",
    "\"classic public domain novel Moby Dick. Your task is to translate the text into \" + \\\n",
    "\"modern American vernacular. You should try to preserve the meaning and setting \" + \\\n",
    "\"of the text, while modernizing its voice. You are only responsible to translate \" + \\\n",
    "\"the portion of the text in the user's prompt, but you must provide a full, \" + \\\n",
    "\"unabbreviated translation of this text. If your response is interrupted, pick up \" + \\\n",
    "\"in the next message exactly where you left off.\"\n",
    "\n",
    "user_prompt = \"If you have completed your translation of the provided text, \" + \\\n",
    "\"respond with 'STOP'. Otherwise, continue your translation from exactly where you \" + \\\n",
    "\"left off.\"\n",
    "\n",
    "# Define LLM parameters\n",
    "models = {\n",
    "        16384: \"gpt-3.5-turbo\",\n",
    "        128000: \"gpt-4-turbo-preview\"\n",
    "    }\n",
    "\n",
    "\n",
    "def is_stop_signal(content: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the response content loosely matches 'STOP', ignoring case and\n",
    "    allowing for some variation in spacing and punctuation.\n",
    "    \"\"\"\n",
    "    # Regular expression to match 'STOP' with flexibility\n",
    "    # \\s* allows for any number of spaces, [.,!?]* allows for trailing punctuation\n",
    "    stop_pattern = re.compile(r\"\\s*STOP\\s*[.,!?]*\\s*$\", re.IGNORECASE)\n",
    "    return bool(stop_pattern.match(content))\n",
    "\n",
    "\n",
    "def prompt_llm(\n",
    "            message_list: list[dict],\n",
    "            minimum_output_tokens: int = 4096,\n",
    "            total_tokens_used: int = 0,\n",
    "            depth: int = 0,\n",
    "            max_depth: int = 16\n",
    "        ) -> tuple[list[dict], int]:\n",
    "    # Check recursion depth\n",
    "    if depth > max_depth:\n",
    "        print(\"Recursion depth limit reached.\")\n",
    "        return message_list, total_tokens_used\n",
    "\n",
    "    print(f\"Recursion depth: {depth}, Message list length: {len(message_list)}, Total tokens used: {total_tokens_used}\")\n",
    "\n",
    "    try:\n",
    "        # Select appropriate model based on prompt length and minimum_output_tokens\n",
    "        model = select_model(message_list, minimum_output_tokens)\n",
    "        \n",
    "        # Prompt the LLM with the current message_list\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=message_list,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        # Remove any text from message that is enclosed in double backslashes\n",
    "        cleaned_message = re.sub(r\"\\\\\\\\.*?\\\\\\\\\", \"\", response.choices[0].message.content)\n",
    "\n",
    "        # Update the message list with the response\n",
    "        if not is_stop_signal(cleaned_message):\n",
    "            message_list.append(\n",
    "                    {\"role\": \"assistant\", \"content\": cleaned_message}\n",
    "                )\n",
    "        \n",
    "        # Update total token usage\n",
    "        total_tokens_used += response.usage.total_tokens\n",
    "\n",
    "        print(\"Response appended to message list.\")\n",
    "\n",
    "        # If response was interrupted due to length, call function recursively with\n",
    "        # updated message_list to prompt again\n",
    "        if response.choices[0].finish_reason == \"length\":\n",
    "            print(\"Response finish reason: length. Recursing with updated message list.\")\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                minimum_output_tokens,\n",
    "                total_tokens_used,\n",
    "                depth + 1,\n",
    "                max_depth\n",
    "            )\n",
    "        \n",
    "        # If content of the response is not 'STOP', append user_prompt message and call\n",
    "        # function recursively with message_list to prompt again\n",
    "        if is_stop_signal(cleaned_message):\n",
    "            print(\"Assistant response is 'STOP'. Returning message list.\")\n",
    "            return message_list, total_tokens_used\n",
    "        else:\n",
    "            print(\"Assistant response is not 'STOP'. Appending user prompt and recursing.\")\n",
    "            message_list.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                minimum_output_tokens,\n",
    "                total_tokens_used,\n",
    "                depth + 1,\n",
    "                max_depth\n",
    "            )            \n",
    "\n",
    "    except Exception as e:\n",
    "        # Try selecting model with more conservative assumptions before raising BadRequestError\n",
    "        if isinstance(e, BadRequestError) and minimum_output_tokens < 4096:\n",
    "            print(\"BadRequestError encountered. Retrying with minimum_output_tokens=4096.\")\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                4096,\n",
    "                total_tokens_used,\n",
    "                depth + 1,\n",
    "                max_depth\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Exception encountered: {e}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "message_list_prompt = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": medium_text}\n",
    "            ]\n",
    "\n",
    "message_list_result, tokens_used = prompt_llm(message_list_prompt, 4096)\n",
    "full_output = get_full_output(message_list_result)\n",
    "print(\"Tokens used:\")\n",
    "print(tokens_used)\n",
    "print(\"Full output:\")\n",
    "print(full_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering for a more accurate 'STOP' signal\n",
    "\n",
    "When we look at the above output, we notice a problem: we fed in 10,000 tokens, but the output is only about 1,000. This isn't (only) because the translation is more concise than the original. It's also because, for some reason, the stopping signal is being thrown prematurely. The translation covers only a small portion of the first chapter, about 5,000 characters of English text.\n",
    "\n",
    "Perhaps this is because we are not allowing the model to reason about whether it has completed the task before returning 'STOP', which allows random chance to determine when the model stops. So, we can try asking the model to reason first about whether it has completed the task. Let's see whether this improves performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursion depth: 0, Message list length: 2, Total tokens used: 0\n",
      "Response appended to message list.\n",
      "Assistant response is not 'STOP'. Appending user prompt and recursing.\n",
      "Recursion depth: 1, Message list length: 4, Total tokens used: 11376\n",
      "Response appended to message list.\n",
      "Assistant response is 'STOP'. Returning message list.\n",
      "Stop reason:  The user's prompt ended with a portion of text from Chapter 3, specifically describing Ishmael's first night at the Spouter-Inn and his contemplations about sharing a bed with the harpooner. My translation concluded with Ishmael being shown to his room by the landlord, which corresponds to the narrative events described in the last paragraph provided by the user. Therefore, the translation of the user's ENTIRE text prompt is complete.\n",
      "Tokens used:\n",
      "23150\n",
      "Full output:\n",
      "CHAPTER 1. Loomings.\n",
      "\n",
      "Just call me Ishmael. A while back—don't worry about the exact time—when I was broke and bored on land, I decided to hit the seas for a change of scenery. Sailing helps me shake off the blues and clear my head. Whenever I start feeling down, or when the weather matches my mood, or when I catch myself lingering near funeral homes, or even when I feel like causing trouble by knocking people's hats off, that's when I know it's time to head out to sea. It's my way of dealing with things. Cato might choose to end it all dramatically, but I prefer a more peaceful approach by setting sail. It's not that unusual. If others knew, most men feel a similar pull towards the ocean at some point.\n",
      "\n",
      "Now picture New York City, surrounded by docks like islands are by coral reefs—commerce crashing against its shores. The streets lead you straight to the water. Down at the southern tip is the Battery, where the waves wash against a sturdy pier, cooled by breezes that just hours ago were far out at sea. Look at the crowds of people watching the water.\n",
      "\n",
      "Take a stroll through the city on a lazy Sunday afternoon. Walk from one end to the other. What do you see? Men standing around lost in thought, gazing out at the ocean. Some leaning against posts, others sitting on the docks, a few peering over the sides of ships from China, and some high up in the rigging, trying to get a better view. But these are all landlubbers; during the week, they're stuck indoors, tied to desks and counters. What's going on here?\n",
      "\n",
      "But wait, here come more people, heading straight for the water, looking like they're about to dive in. Odd, right? They can't seem to get close enough to the water, hanging out near the warehouses just won't cut it. No, they need to be as close to the water as possible without falling in. And there they stand, miles of them, coming from all directions—north, east, south, west. Yet here they all gather. Is there some magnetic force drawing them to the ships' compass needles?\n",
      "\n",
      "Let's try this. Imagine you're in the countryside, surrounded by lakes. Take any path and chances are it will lead you to a stream. There's something magical about it. Even the most absent-minded person, lost in thought, will somehow find their way to water. Water and meditation go hand in hand.\n",
      "\n",
      "Now, picture an artist. He wants to paint the dreamiest, most peaceful landscape in the Saco Valley. What's his secret? Picture the trees with hollow trunks, a meadow where cattle graze, smoke rising from a cottage, and a winding path leading into the distant woods towards blue mountains. But all of this would be meaningless without the magic stream in front of him. Visit the Prairies in June, surrounded by Tiger-lilies for miles—what's missing? Water. Without water, even Niagara Falls would lose its appeal. Why did a poet from Tennessee, upon receiving silver, debate between buying a coat or going on a trip to the beach? Why do healthy boys dream of going to sea? Why did you feel a mystical connection on your first voyage when told you were out of sight of land? Why did ancient Persians revere the sea? Why did the Greeks give it a deity status, a sibling of Jove? There's meaning behind all of this. And deeper still is the story of Narcissus, unable to grasp his reflection in the water, diving in and drowning. That same reflection is seen in rivers and oceans everywhere. It's the unattainable essence of life, the key to it all.\n",
      "\n",
      "When I say I head to sea when my eyes get hazy and my lungs feel heavy, I don't mean as a passenger. To be a passenger, you need money in your pocket, and a purse is useless without something in it. Passengers get seasick, they argue, they can't sleep, they don't enjoy it much. No, I don't go as a passenger. Even though I know my way around a ship, I don't go as a Commodore, a Captain, or a Cook. I leave those roles to those who enjoy them. I prefer to take care of myself without the added responsibility of ships. And as for being a cook—sure, there's glory in it, but I never fancied grilling chickens. But once they're grilled to perfection, there's no one who appreciates a good broiled fowl more than I do. The ancient Egyptians may have worshipped broiled ibis and roasted river horse, but you won't find me idolizing them in their pyramids.\n",
      "\n",
      "No, when I go to sea, I go as a simple sailor, working hard on deck and up in the crow's nest. They give me orders, make me jump around like a grasshopper. At first, it's not pleasant. It challenges my sense of honor, especially coming from a respected family like the Van Rensselaers or Randolphs. It's a sharp transition from being a schoolmaster to a sailor, requiring a dose of Stoicism to handle it. But with time, even that wears off.\n",
      "\n",
      "So what if an old sea captain tells me to sweep the decks? What's the harm in that, really? In the grand scheme of things, does it matter much, especially when compared to the teachings of the New Testament? Do you think the archangel Gabriel would think less of me for obeying that old sea captain? Who isn't a servant in some way? Well, however those sea captains may order me around, however they may push and prod me, I find solace in knowing that it's all part of the job. Everyone gets their fair share of physical or metaphysical challenges, and so we all endure together, lending a hand to one another.\n",
      "\n",
      "And another reason I go to sea as a sailor is because they pay me for my work, unlike passengers who have to pay their way. There's a world of difference between paying and being paid. Paying can be uncomfortable, like an affliction forced upon us. But being paid—what a feeling! The way a person receives money with such grace is remarkable, considering we often believe money is the root of all evil and that a wealthy person can't enter heaven. How willingly we accept our fate!\n",
      "\n",
      "Lastly, I go to sea for the physical activity and fresh air on the deck. Just like in life, headwinds are more common than tailwinds, so the Commodore on deck mostly benefits from the sailors' atmosphere. He thinks he's breathing it first, but really, it's secondhand. In many ways, the common folk lead their leaders, without them even realizing it. But why, after years of smelling the sea as a merchant sailor, did I suddenly decide to go on a whaling voyage? The mysterious forces of fate, watching over me, guided me towards this journey. It was all part of a grand plan, a brief interlude before bigger events. It was like a playbill with a mix of events: a presidential election, a whaling voyage by one Ishmael, and a bloody battle in Afghanistan.\n",
      "\n",
      "I couldn't tell you why the Fates chose me for this humble whaling voyage, while others were cast in grand roles in high tragedies or light comedies. But looking back, I can see the hidden motives that led me to take on this role, cleverly disguised and presented to me, making me believe it was my own choice, free from bias and judgment.\n",
      "\n",
      "The main reason behind it all was the great whale itself. This monstrous and mysterious creature piqued my curiosity. The vast and distant seas where it roamed, the unknown dangers it posed, and the incredible sights and sounds of Patagonia all played a part in drawing me towards this adventure. While others might not have been swayed by these things, I, with my insatiable thirst for the unknown, was quick to embrace the horror and even find a strange comfort in it—if they would let me—since it's wise to be on good terms with all the inhabitants of the place you find yourself in.\n",
      "\n",
      "Because of these reasons, the whaling voyage beckoned me. The floodgates of wonder opened wide, and in my wild dreams and fantasies, endless processions of whales floated into my soul, with one grand hooded phantom towering above them all.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"The user will provide a portion of the text of Herman Melville's \" + \\\n",
    "\"classic public domain novel Moby Dick. Your task is to translate the text into \" + \\\n",
    "\"modern American vernacular. You should try to preserve the meaning and setting \" + \\\n",
    "\"of the text, while modernizing its voice. Each chapter and each paragraph in the \" + \\\n",
    "\"original text should have a corresponding chapter or paragraph in the \" + \\\n",
    "\"translation. You are only responsible to translate the portion of the text in \" + \\\n",
    "\"the user's prompt, but you must provide a full, unabbreviated translation of \" + \\\n",
    "\"this text. If your response is interrupted, pick up in the next message exactly \" + \\\n",
    "\"where you left off.\"\n",
    "\n",
    "user_prompt = \"Begin your next response by reasoning about how much of the text \" + \\\n",
    "\"in the user's prompt you have already translated, and where in that text \" + \\\n",
    "\"you left off. You must enclose your reasoning in \\\\\\\\double backslashes\\\\\\\\ to \" + \\\n",
    "\"distinguish it from your output. After your reasoning block, respond with 'STOP' \" + \\\n",
    "\"if, and only if, you have completed your full, unabbreviated translation of \" + \\\n",
    "\"the user's ENTIRE text prompt. Consider what chapter the user's prompt ended \" + \\\n",
    "\"in, and whether your translation has yet reached that chapter. Also consider \" + \\\n",
    "\"whether the specific narrative events of the final paragraph of the user's \" + \\\n",
    "\"prompt crrespond to the specific narrative events described in the last paragraph \" + \\\n",
    "\"of your translated text. Only return 'STOP' if both these conditions are met. \" + \\\n",
    "\"Examples:\\n\\n\" + \\\n",
    "\"\\\\\\\\The text in the user's prompt ended with a description of the carpenter \" + \\\n",
    "\"making Ahab a peg leg in Chapter 145. My translation concluded in Chapter 145 \" + \\\n",
    "\"with a corresponding description of the carpenter making Ahab a wooden \" + \\\n",
    "\"prosthetic. Thus, the translation of the provided text is complete.\\\\\\\\STOP\\n\\n\" + \\\n",
    "\"\\\\\\\\The text in the user's prompt broke off in the middle of chapter 5. I \" + \\\n",
    "\"concluded my last message with a translation of Ishmael asking the landlord if \" + \\\n",
    "\"the harpooner always keeps such late hours in the middle of chapter 3. The \" + \\\n",
    "\"translation of the provided text has not yet reached chapter 5, so I will \" + \\\n",
    "\"continue from where I left off.\\\\\\\\Narrative continues here.\"\n",
    "\n",
    "# Define LLM parameters\n",
    "models = {\n",
    "        16384: \"gpt-3.5-turbo\",\n",
    "        128000: \"gpt-4-turbo-preview\"\n",
    "    }\n",
    "\n",
    "\n",
    "def is_stop_signal(content: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the response content loosely matches 'STOP', ignoring case and\n",
    "    allowing for some variation in spacing and punctuation.\n",
    "    \"\"\"\n",
    "    # Regular expression to match 'STOP' with flexibility\n",
    "    # \\s* allows for any number of spaces, [.,!?]* allows for trailing punctuation\n",
    "    stop_pattern = re.compile(r\"\\s*STOP\\s*[.,!?]*\\s*$\", re.IGNORECASE)\n",
    "    return bool(stop_pattern.match(content))\n",
    "\n",
    "\n",
    "clean_output = lambda content: re.sub(r\"\\\\\\\\.*?\\\\\\\\\", \"\", content)\n",
    "\n",
    "\n",
    "def prompt_llm(\n",
    "            message_list: list[dict],\n",
    "            minimum_output_tokens: int = 4096,\n",
    "            total_tokens_used: int = 0,\n",
    "            depth: int = 0,\n",
    "            max_depth: int = 3\n",
    "        ) -> tuple[list[dict], int]:\n",
    "    # Check recursion depth\n",
    "    if depth > max_depth:\n",
    "        print(\"Recursion depth limit reached.\")\n",
    "        return message_list, total_tokens_used\n",
    "\n",
    "    print(f\"Recursion depth: {depth}, Message list length: {len(message_list)}, Total tokens used: {total_tokens_used}\")\n",
    "\n",
    "    try:\n",
    "        # Select appropriate model based on prompt length and minimum_output_tokens\n",
    "        model = select_model(message_list, minimum_output_tokens)\n",
    "        \n",
    "        # Prompt the LLM with the current message_list\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=message_list,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        # Remove any text from message that is enclosed in double backslashes\n",
    "        cleaned_message = clean_output(response.choices[0].message.content)\n",
    "\n",
    "        # Update the message list with the response\n",
    "        if not is_stop_signal(cleaned_message):\n",
    "            message_list.append(\n",
    "                    {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "                )\n",
    "        \n",
    "        # Update total token usage\n",
    "        total_tokens_used += response.usage.total_tokens\n",
    "\n",
    "        print(\"Response appended to message list.\")\n",
    "\n",
    "        # If response was interrupted due to length, call function recursively with\n",
    "        # updated message_list to prompt again\n",
    "        if response.choices[0].finish_reason == \"length\":\n",
    "            print(\"Response finish reason: length. Recursing with updated message list.\")\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                minimum_output_tokens,\n",
    "                total_tokens_used,\n",
    "                depth + 1,\n",
    "                max_depth\n",
    "            )\n",
    "        \n",
    "        # If content of the response is not 'STOP', append user_prompt message and call\n",
    "        # function recursively with message_list to prompt again\n",
    "        if is_stop_signal(cleaned_message):\n",
    "            print(\"Assistant response is 'STOP'. Returning message list.\")\n",
    "            print(\"Stop reason: \", re.findall(r\"\\\\\\\\(.*?)\\\\\\\\\", response.choices[0].message.content)[0])\n",
    "            return message_list, total_tokens_used\n",
    "        else:\n",
    "            print(\"Assistant response is not 'STOP'. Appending user prompt and recursing.\")\n",
    "            message_list.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                minimum_output_tokens,\n",
    "                total_tokens_used,\n",
    "                depth + 1,\n",
    "                max_depth\n",
    "            )            \n",
    "\n",
    "    except Exception as e:\n",
    "        # Try selecting model with more conservative assumptions before raising BadRequestError\n",
    "        if isinstance(e, BadRequestError) and minimum_output_tokens < 4096:\n",
    "            print(\"BadRequestError encountered. Retrying with minimum_output_tokens=4096.\")\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                4096,\n",
    "                total_tokens_used,\n",
    "                depth + 1,\n",
    "                max_depth\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Exception encountered: {e}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "message_list_prompt = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": medium_text}\n",
    "            ]\n",
    "\n",
    "message_list_result, tokens_used = prompt_llm(message_list_prompt, 4096)\n",
    "full_output = clean_output(get_full_output(message_list_result))\n",
    "print(\"Tokens used:\")\n",
    "print(tokens_used)\n",
    "print(\"Full output:\")\n",
    "print(full_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a *little* further, but even after *extensive* prompt engineering, including providing examples, the model still cannot consistently determine how much of the task it has completed or how far its translation has progressed. Specifically, the chapter breaks seem to pose a problem, because the model wants to return a 'STOP' signal at the end of each chapter. Our prompt continues through Chapter 3, yet we can't seem to consistently get out of Chapter 1.\n",
    "\n",
    "I'm honestly a little stunned that the model is this stubbornly dumb, but I suppose it's kind of a known issue. A known limitation of OpenAI's models is that they're not great at searching over large context; they tend to put too much weight on the beginning and end of the context. They also tend not to be very good at planning or executive function tasks such as making the decision whether to stop or continue. And, of course, they're notoriously innumerate, so they can't count paragraphs or even, apparently, tell the difference between Chapters 1 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbering paragraphs to improve performance\n",
    "\n",
    "It's likely, I think, that we could get better results on our stopping task with GPT-4, but since we're specifically trying to take of advantage of the Turbo model's long context, let's instead see if we can improve the results by numbering paragraphs to help the model keep track of its progress without having to count paragraphs or chapters or identify corresponding narrative events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 This is a test.\n",
      "2 This is another test.\n",
      "This is a test.\n",
      "\n",
      "This is another test.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def number_lines(text: str) -> str:\n",
    "    # Replace double line breaks with single line breaks\n",
    "    text = text.replace(\"\\n\\n\", \"\\n\")\n",
    "    # Split the text by line breaks\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    # Number the paragraphs\n",
    "    numbered_paragraphs = [f\"{i+1} {paragraph}\" for i, paragraph in enumerate(paragraphs)]\n",
    "    # Join the paragraphs back together\n",
    "    return \"\\n\".join(numbered_paragraphs)\n",
    "\n",
    "def unnumber_lines(text: str) -> str:\n",
    "    # Split the text by line breaks\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    # Remove the paragraph numbers\n",
    "    unnumbered_paragraphs = [re.sub(r\"^\\d+ \", \"\", paragraph) for paragraph in paragraphs]\n",
    "    # Join the paragraphs back together\n",
    "    return \"\\n\\n\".join(unnumbered_paragraphs)\n",
    "\n",
    "def get_last_line_number(text: str) -> int:\n",
    "    # Split the text by line breaks\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    # Get the last paragraph number\n",
    "    return int(paragraphs[-1].split(\" \")[0])\n",
    "\n",
    "print(number_lines(\"This is a test.\\n\\nThis is another test.\"))\n",
    "print(unnumber_lines(\"1 This is a test.\\n2 This is another test.\"))\n",
    "print(get_last_line_number(\"1 This is a test.\\n2 This is another test.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should, in fact, altogether eliminate the need to re-prompt the model for a stopping signal, because we can simply automate comparison of the input and output paragraph numbers to determine whether the model has completed the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursion depth: 0, Message list length: 2, Total tokens used: 0\n",
      "Response appended to message list.\n",
      "Response finish reason: length. Recursing with updated message list.\n",
      "Recursion depth: 1, Message list length: 3, Total tokens used: 13833\n",
      "Response appended to message list.\n",
      "Translation complete. Returning message list.\n",
      "Tokens used:\n",
      "29729\n",
      "Full output:\n",
      "CHAPTER 1. Loomings.\n",
      "\n",
      "Just call me Ishmael. A while back—can't say exactly how long—when I was broke as a joke and had nothing going on ashore, I figured I'd hit the high seas for a bit. It's my way of shaking off the blues and getting some fresh air. Whenever I start feeling all gloomy and down, like a wet, dreary November day in my soul; whenever I catch myself staring at coffin shops and following funerals; and especially when my bad moods start making me want to start some trouble, like knocking people's hats off for no reason—well, that's when I know it's time to head out to sea. It's my way of clearing my head. While some folks might turn to drastic measures like suicide, I opt for a sea voyage instead. It's no big deal. If people knew, most guys feel pretty much the same way about the ocean at some point in their lives.\n",
      "\n",
      "Now picture this: New York City, surrounded by docks like a tropical island by coral reefs—commerce has its grip on it. Everywhere you look, the streets lead you to the water. Down at the southern tip is the Battery, where the waves wash against a noble pier, cooled by breezes that just a few hours ago were out at sea. Look at all the folks gazing out at the water.\n",
      "\n",
      "Take a stroll through the city on a lazy Sunday afternoon. Walk from Corlears Hook to Coenties Slip, and then up towards Whitehall. What do you see? People standing around like statues, lost in thoughts of the ocean. Some leaning against the posts, some sitting on the pier, some peering over the sides of ships from China, some high up in the rigging trying to get a better view out to sea. But these are all landlubbers; during the week, they're stuck indoors, tied to their jobs—behind counters, nailed to benches, chained to desks. How did they end up here?\n",
      "\n",
      "But look! Here come more crowds, heading straight for the water, like they're about to dive right in. Strange! They won't be satisfied with just hanging out in the shade near those warehouses. No, they have to get as close to the water as possible without falling in. And there they stand—miles of them—endless rows. They come from all over, from every corner of the city—north, east, south, and west. Yet here they all gather. Is there some magnetic force drawing them to the compass needles of those ships?\n",
      "\n",
      "Picture this: You're out in the country, up in some high lake country. Take any path you like, and chances are it'll lead you down to a valley, by a pool in a stream. There's something magical about it. Even the most absent-minded person, lost in thought, if you stand him up and set him walking, he'll inevitably lead you to water, if there's any around. If you ever find yourself thirsty in the middle of the American desert, try this out, if you happen to have a philosopher in your group. Yes, as everyone knows, meditation and water are inseparable.\n",
      "\n",
      "Now, imagine an artist. He wants to paint the most dreamy, peaceful, enchanting landscape in the Saco Valley. What's the key element he uses? There are the trees, each with a hollow trunk, as if a hermit and a cross were inside; there's the meadow, the cattle, the smoke rising from a cottage. A winding path leads deep into the woods, reaching the mountains bathed in their blue hues. But even with all this beauty, it would be meaningless without the shepherd's gaze fixed on the magical stream before him. Visit the Prairies in June, where you wade through fields of Tiger-lilies for miles—what's missing? Water—there's not a drop to be found! If Niagara Falls were just a sand dune, would you travel a thousand miles to see it? Why did the poet from Tennessee, upon receiving a bit of money, debate whether to buy a coat he desperately needed or take a trip to Rockaway Beach? Why does every healthy, adventurous boy at some point dream of going to sea? Why did you feel a mysterious thrill on your first voyage when told you were out of sight of land? Why did the ancient Persians revere the sea? Why did the Greeks give it a godly status, a brother of Zeus? Surely, there's a deeper meaning to all this. And the story of Narcissus, unable to grasp the image he saw in the fountain and drowning in it, holds a truth we see in all rivers and oceans. It's the image of the unattainable essence of life; that's the key to it all.\n",
      "\n",
      "When I say I head to sea whenever my eyes get hazy and my lungs feel heavy, I don't mean I go as a passenger. To be a passenger, you need money, and money's useless without something to spend it on. Plus, passengers get seasick, get cranky, can't sleep well, don't have much fun in general; no, I never go as a passenger. Even though I know my way around a ship, I don't go as a Commodore, a Captain, or a Cook. I leave the glory and responsibilities of those roles to those who enjoy them. Personally, I can barely take care of myself, let alone ships of all kinds. And as for being a cook—sure, there's some glory in that, being an officer on board—but I never really fancied grilling chickens; even though once they're grilled, buttered just right, and seasoned perfectly, there's no one who appreciates a grilled chicken more than I do. It's like the ancient Egyptians' obsession with grilled ibis and roasted river horse, leading to mummified creatures in the pyramids.\n",
      "\n",
      "No, when I go to sea, I go as a simple sailor, working my way from the front of the ship down to the forecastle, up to the royal mast-head. Sure, they boss me around a bit, make me jump from one spar to another like a grasshopper in a field. At first, it's not the most pleasant experience. It challenges my sense of honor, especially coming from a respected family like the Van Rensselaers or Randolphs. And especially if, just before dipping my hand in the tar pot, I was acting all high and mighty as a country schoolteacher, making the big boys fear me. The transition is tough, believe me, going from a teacher to a sailor, it takes some serious mental strength. But you get used to it over time.\n",
      "\n",
      "So what if some old sea captain tells me to grab a broom and sweep the decks? What's the big deal, really, when you think about it in the grand scheme of things? Do you think the archangel Gabriel would think any less of me because I obediently sweep the deck for that old sea captain? Who isn't a servant in some way? Well, no matter how those old sea captains might order me around, thump and push me, I find comfort in knowing it's all part of the deal; everyone else gets treated the same way in one form or another—whether physically or mentally, that is. So, the universal thump goes around, and we should all just pat each other on the back and be content.\n",
      "\n",
      "On the other hand, when I go to sea, I get paid for my work, unlike passengers who have to pay their way. And there's a world of difference between paying and being paid. Paying is probably the most uncomfortable thing we have to endure. But being paid—what could be better? The way a person receives money with such grace is quite remarkable, considering we often believe money is the root of all evil, and that a rich man can't enter heaven. Ah, how willingly we accept our damnation!\n",
      "\n",
      "Lastly, I go to sea as a sailor for the healthy exercise and fresh air on the forecastle deck. Just like in life, headwinds are more common than tailwinds (if you follow the Pythagorean principle), so the Commodore on the quarter-deck mostly gets his air secondhand from the sailors up front. He thinks he's breathing it first, but not really. In the same way, common folks often lead their leaders in many things, without the leaders even realizing it. But why, after smelling the sea as a merchant sailor, did I suddenly decide to go on a whaling voyage? The invisible hand of Fate, always watching over me, guiding me in some mysterious way, can answer that better than anyone else. Undoubtedly, my decision to go whaling was part of a grand plan laid out long ago by Providence. It was like a brief interlude between more significant events. I imagine the bill for that part of my life must have read something like this:\n",
      "\n",
      "\"Grand Contested Election for the Presidency of the United States. \"WHALING VOYAGE BY ONE ISHMAEL. \"BLOODY BATTLE IN AFFGHANISTAN.\"\n",
      "\n",
      "Although I can't explain why the Fates cast me in the role of a whaling voyage when others were assigned grand parts in high tragedies or easy roles in comedies, now that I look back, I can see the motives that led me to take on the role I did. The circumstances were presented to me in various disguises, nudging me towards a path that seemed like my own choice, influenced by my supposedly unbiased free will and discerning judgment.\n",
      "\n",
      "The main driving force behind my decision was the overwhelming presence of the great whale itself. This monstrous and mysterious creature piqued my curiosity like nothing else. The vast and remote seas where it roamed, the nameless dangers of whaling, along with the countless wonders of Patagonian sights and sounds, all played a part in convincing me to pursue this adventure. While others might not have been swayed by these things, for me, there's an insatiable desire for the unknown. I crave to sail forbidden waters and explore wild shores. Not that I ignore the good things in life, but I'm quick to recognize the horrors too, and I can still find a connection with them—if they'd let me—since it's wise to be on good terms with all the inhabitants of the places we find ourselves in.\n",
      "\n",
      "Because of these reasons, the whaling voyage beckoned me; the floodgates to a world of wonders swung open, and in the wild fantasies that drove me forward, visions of whales paraded through my mind, with one grand hooded phantom towering above them all, like a snow-capped mountain in the sky.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER 2. The Carpet-Bag.\n",
      "\n",
      "I packed a couple of shirts into my old carpet bag, slung it over my shoulder, and set off for Cape Horn and the Pacific. Leaving the bustling city of old Manhattan, I eventually arrived in New Bedford. It was a chilly Saturday night in December. I was disappointed to learn that the little boat to Nantucket had already left, and there was no way to reach the island until Monday.\n",
      "\n",
      "Most young folks looking to join a whaling crew stop in New Bedford before setting off on their voyage. I, on the other hand, had no intention of doing so. I was dead set on sailing on a Nantucket ship, drawn to the rugged charm of that famous island. Even though New Bedford had been taking over the whaling business lately, leaving poor old Nantucket behind, it was Nantucket that started it all—the birthplace of American whaling. Where else but Nantucket did the Native American whalemen, the Red-Men, first venture out in canoes to hunt the Leviathan? And where else but Nantucket did that brave little sloop set sail, loaded with imported cobblestones—legend has it—to toss at whales to see if they were close enough to harpoon from the bow?\n",
      "\n",
      "With a night, a day, and another night to kill in New Bedford before I could head to my final destination, I needed to figure out where to eat and sleep. It was a dark and dreary night, bitterly cold and desolate. I didn't know anyone in town. After checking my pockets and finding only a few coins, I told myself, \"Wherever you end up, Ishmael, be sure to ask about the price, and don't be too picky.\"\n",
      "\n",
      "I wandered the streets, passing by \"The Crossed Harpoons\" inn, which looked too fancy and pricey for my taste. Further down, the \"Sword-Fish Inn\" had bright red windows that seemed to melt the snow and ice in front of the house, while the rest of the street was frozen solid. My worn-out boots made it uncomfortable to walk on the icy ground. \"Too fancy and expensive,\" I thought, watching the warm glow and hearing the clinking glasses inside. \"Keep moving, Ishmael,\" I finally said to myself, stepping away from the door. \"Your boots are blocking the way.\" So, I continued on. I instinctively followed the streets leading towards the water, knowing that the inns there would likely be the cheapest and most welcoming.\n",
      "\n",
      "The streets were dreary, with dark shadows and flickering candlelight here and there. At this late hour on a Saturday night, the town seemed deserted. But soon, I spotted a dim light not far from the docks, accompanied by a creaking sound in the air. Looking up, I saw a sign swinging over the door, painted with a faint image of a tall spray of mist, and the words \"The Spouter Inn: Peter Coffin\" underneath.\n",
      "\n",
      "Stepping inside, it felt like I had entered a dark and eerie place. A hundred black faces turned to look at me, and at the front, a preacher was delivering a sermon about darkness and despair. \"Ha, Ishmael,\" I muttered to myself, backing out slowly, \"Not the most welcoming place at 'The Trap' inn.\"\n",
      "\n",
      "Moving on, I eventually found a dimly lit spot near the docks, where I heard a creaking noise in the air. Looking up, I saw a sign with a painting of a misty spray, and the words \"The Spouter Inn: Peter Coffin\" written below.\n",
      "\n",
      "Coffin? Spouter? Those names seemed ominous in that context. But I had heard that Coffin was a common name in Nantucket, and I assumed this Peter was an immigrant from there. The dim light and the worn-down wooden house gave it a gloomy feel, as if it had been salvaged from the ruins of a fire. The swinging sign made a creaking sound, adding to the eerie atmosphere. I thought this must be the place for affordable lodging and a decent cup of coffee.\n",
      "\n",
      "It was an odd place, with a crooked old house that seemed to lean sadly on one side. Located on a sharp, bleak corner where the wind howled fiercely, it was a stark contrast to the warm and inviting atmosphere inside. \"In judging the tempestuous wind called Euroclydon,\" as an old writer once said, \"it makes a marvelous difference whether you look at it from a glass window with frost on the outside, or from a window with frost on both sides, where Death is the only glazier.\" I couldn't help but agree with that sentiment, as I settled in for the night.\n",
      "\n",
      "But what about Lazarus? Would he rather be in Sumatra than here? Would he prefer lying along the equator or facing the frosty winds of Euroclydon? The image of Lazarus stranded outside Dives' door was more astonishing than an iceberg moored to the Moluccas. Yet Dives himself lived like a king in an ice palace made of frozen sighs, sipping on the tears of orphans as a president of a temperance society.\n",
      "\n",
      "Enough of these thoughts for now; we're off to hunt whales, and there's plenty more to come. Let's shake off the ice from our feet and see what awaits us at the Spouter Inn.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER 3. The Spouter-Inn.\n",
      "\n",
      "Stepping into the gable-ended Spouter-Inn, you found yourself in a wide, low entryway with old-fashioned walls that reminded you of the hull of a condemned ship. On one side hung a large oil painting so smudged and defaced that it took some serious effort to figure out what it depicted. The shadows and shades were so puzzling that at first, it seemed like a young artist, back in the days of New England witch trials, had tried to capture bewitched chaos on canvas. But after much contemplation and study, and by peering out the small window at the back of the entryway, you could finally make out a semblance of its purpose.\n",
      "\n",
      "The most puzzling part of the painting was a long, ominous black mass hovering in the center, above three faint blue lines floating in a mysterious mist. It was a swampy, eerie image that could drive a nervous man to madness. Yet, there was a strange, almost sublime quality about it that held you in its grip, making you swear to uncover its meaning. Every now and then, a bright but deceptive idea would flash through your mind. \"It's the Black Sea in a stormy night.\" \"It's the battle of the elements.\" \"It's a desolate heath.\" \"It's a scene from a frozen land.\" \"It's the passage of time breaking free from ice.\" But eventually, all these notions gave way to the central, enigmatic presence in the painting. Once you grasped that, everything else fell into place. But hold on; doesn't it bear a faint resemblance to a massive fish? Perhaps even the great whale itself?\n",
      "\n",
      "In fact, the artist's intention seemed to be this: a theory of my own, partly based on the collective opinions of many elderly folks I spoke to. The painting portrays a Cape-Horner in a fierce storm; the half-sunken ship struggling with its three broken masts visible; and an enraged whale, aiming to leap over the vessel, is in the colossal act of impaling itself on the mastheads.\n",
      "\n",
      "The opposite wall of the entryway was covered with an array of heathenish clubs and spears. Some were studded with sharp teeth resembling ivory saws; others were adorned with knots of human hair; and one had a sickle shape, with a massive handle curving like a swath cut by a long-armed mower. It sent shivers down your spine, wondering what kind of monstrous cannibal or savage had wielded such a horrifying weapon. Among these were rusty old whaling lances and harpoons, broken and twisted. Some had stories attached to them. With one long lance, now bent out of shape, Nathan Swain had killed fifteen whales between sunrise and sunset fifty years ago. And that harpoon, now twisted like a corkscrew, was thrown in the waters of Java, taken by a whale, and years later killed off the coast of Blanco. The original iron pierced near the tail, traveling forty feet like a restless needle before embedding in the hump.\n",
      "\n",
      "Crossing this dim entryway and passing through a low-arched passage, which seemed to have once been a central chimney with fireplaces all around, you entered the main room. It was even darker, with heavy beams above and old, weathered planks below, making you feel like you were in the belly of an old ship, especially on a stormy night when the creaking ark rocked violently. On one side stood a long, low table covered with cracked glass cases, filled with dusty oddities from the far corners of the world. From another corner of the room, a dark den—the bar—loomed, resembling a crude attempt at a right whale's head. Whether it was meant to be that or not, the massive arched bone of a whale's jaw stood there, wide enough for a carriage to pass under. Inside, shelves lined with old decanters, bottles, and flasks surrounded a small, withered old man, who, for a price, sold sailors their doom in a glass.\n",
      "\n",
      "The glasses he poured his poison into were abominable. They looked like proper cylinders from the outside, but inside, they tapered down deceitfully to a narrow bottom. Roughly etched lines circled these trickster's cups, marking different prices for different levels of fill. A sip might cost a penny, a gulp a penny more, up to the full glass—the Cape Horn measure—that you could down for a shilling.\n",
      "\n",
      "When I walked in, I found a group of young sailors gathered around a table, inspecting various pieces of scrimshaw by the dim light. I approached the landlord, asking for a room, only to be told the inn was full. \"But hold on,\" he added, tapping his forehead, \"you wouldn't mind sharing a bed with a harpooneer, would you? You're off to sea, so you might as well get used to it.\"\n",
      "\n",
      "I told him I wasn't keen on sharing a bed, but if the harpooneer wasn't too objectionable and there really was no other option, I'd rather bunk with him than wander around a strange town on such a cold night.\n",
      "\n",
      "\"Thought so. All right, take a seat. Hungry? Supper will be ready soon.\"\n",
      "\n",
      "I sat on an old wooden bench, intricately carved, much like those found on the Battery. At one end, a sailor was busy carving a ship into the wood, focused intently on his work.\n",
      "\n",
      "Eventually, a few of us were called to supper in the next room. It was freezing—no fire, just two dim candles. We huddled in our coats, sipping scalding tea to warm our fingers. The meal was hearty, though—meat, potatoes, and even dumplings. One young guy in a green coat attacked his dumplings with such gusto.\n",
      "\n",
      "\"You'll have nightmares for sure,\" the landlord warned him.\n",
      "\n",
      "I whispered to the landlord, asking if the harpooneer was around.\n",
      "\n",
      "\"Oh, no,\" he replied with a mischievous grin, \"the harpooneer's a dark-skinned fellow. He doesn't eat dumplings—he only eats steak, and rare at that.\"\n",
      "\n",
      "\"Really?\" I said. \"Where is he? Is he here?\"\n",
      "\n",
      "\"He'll be here soon enough,\" was the reply.\n",
      "\n",
      "I couldn't help feeling a bit wary about this \"dark-skinned\" harpooneer. I decided that if we were to share a bed, he'd have to get in first.\n",
      "\n",
      "After supper, everyone returned to the bar. Not knowing what else to do, I decided to just watch the evening unfold.\n",
      "\n",
      "Suddenly, a commotion erupted outside. The landlord announced it was the crew of the Grampus, back from a three-year voyage with a full ship. \"Now we'll get the latest news from the Feegees,\" he exclaimed.\n",
      "\n",
      "The door burst open, and in came a wild bunch of sailors, wrapped in their shaggy coats, faces hidden by woolen wraps, looking like a band of bears from Labrador. They had just come ashore, and this was their first stop. No surprise they headed straight for the bar, where the old Jonah behind the counter quickly served them rounds of drinks. One sailor complained of a cold, and Jonah whipped up a potent mix of gin and molasses, claiming it cured all colds, no matter where you caught them.\n",
      "\n",
      "The alcohol quickly went to their heads, as it often does with sailors fresh off a voyage, and they started to get rowdy.\n",
      "\n",
      "However, one of them stood apart, trying not to dampen his mates' spirits with his sober demeanor, yet choosing not to join in their raucous behavior. This man caught my interest immediately, and since fate would have it that he'd soon become my shipmate (though only a sleeping partner in this narrative), I'll take a moment to describe him. He was tall, with broad shoulders and a chest like a fortress. His deeply tanned face made his white teeth stand out, and his eyes held a hint of sadness. His voice revealed his Southern origins, and his stature suggested he might be one of those tall mountaineers from Virginia's Alleghany Ridge. When the party reached its peak, he slipped away unnoticed, and I didn't see him again until we were at sea. But soon, his absence was noted, and his shipmates, who clearly adored him, started calling for \"Bulkington! Bulkington!\" and rushed out to find him.\n",
      "\n",
      "It was now around nine o'clock, and the room felt eerily quiet after the sailors' departure. I began to feel relieved about a plan I had concocted just before they arrived.\n",
      "\n",
      "No one likes sharing a bed, especially with a stranger. There's something about sleeping that people prefer to do privately. And the idea of sharing a bed with an unknown harpooneer, in a strange inn, in a strange town, was even less appealing. There was no reason for me, a sailor, to share a bed any more than anyone else; sailors have their own hammocks at sea, after all.\n",
      "\n",
      "The more I thought about the harpooneer, the less I liked the idea of sharing a bed with him. It was safe to assume his hygiene might not be up to par. I started to feel uncomfortable all over. Plus, it was getting late, and any decent harpooneer should have been back and heading to bed by now. What if he stumbled in drunk in the middle of the night?\n",
      "\n",
      "\"Landlord, I've changed my mind about the harpooneer. I won't share a bed with him. I'll try the bench.\"\n",
      "\n",
      "\"Suit yourself,\" the landlord replied, \"though I can't offer you much in the way of comfort. This bench is pretty rough.\" He then suggested using a carpenter's plane to smooth it out a bit.\n",
      "\n",
      "After some effort with the plane, which only ended up revealing more knots in the wood, the landlord gave up, and I resigned myself to a less than comfortable night on the bench.\n",
      "\n",
      "I measured the bench and found it too short and narrow for a good night's sleep. I tried to arrange it against the wall for a bit of support, but the cold drafts from the window and door made it impossible to settle down.\n",
      "\n",
      "Frustrated, I briefly considered locking the harpooneer out of his room and taking his bed for myself. But I dismissed the idea, fearing the confrontation that might follow in the morning.\n",
      "\n",
      "As the night wore on, and other boarders came in and went to bed, there was still no sign of the harpooneer.\n",
      "\n",
      "\"Landlord, what's the deal with this guy? Does he always come in this late?\" It was nearly midnight.\n",
      "\n",
      "The landlord chuckled, seemingly amused by something I didn't understand. \"No,\" he said, \"he's usually an early riser. But tonight, he's out selling something, and I can't imagine what's keeping him.\"\n",
      "\n",
      "\"Selling something? On a Saturday night?\" I was getting frustrated. \"What's he selling, his head?\"\n",
      "\n",
      "\"Exactly,\" the landlord replied, \"and I told him he wouldn't find any buyers here. The market's oversaturated.\"\n",
      "\n",
      "\"With what?\" I demanded.\n",
      "\n",
      "\"With heads, of course. Aren't there enough heads in the world?\"\n",
      "\n",
      "I tried to keep my cool. \"Landlord, you better stop messing with me. I'm not falling for it.\"\n",
      "\n",
      "\"Maybe not,\" he said, whittling a toothpick, \"but you might not want to talk so loudly about the harpooneer's business. He might not take kindly to it.\"\n",
      "\n",
      "\"I'll deal with him if I have to,\" I said, my patience wearing thin.\n",
      "\n",
      "\"It's already dealt with,\" the landlord said cryptically.\n",
      "\n",
      "\"What do you mean, 'dealt with'?\" I asked, confused.\n",
      "\n",
      "\"Exactly that. And that's why he can't sell it, I suppose.\"\n",
      "\n",
      "Frustrated, I demanded a straight answer from the landlord. I wanted to know who this harpooneer was and whether it was safe to share a room with him. And I insisted he clarify the bizarre story about selling his head.\n",
      "\n",
      "The landlord finally explained that the harpooneer had been in the South Seas, buying up preserved New Zealand heads, a sort of hobby of his. He'd sold all but one and was trying to sell the last one before Sunday. The landlord had stopped him from peddling heads on a previous Sunday, emphasizing the inappropriateness of such a trade on the Sabbath.\n",
      "\n",
      "This explanation cleared up the mystery but left me wondering about the harpooneer's character. Selling human heads on a Saturday night seemed a bit too macabre for my taste.\n",
      "\n",
      "\"Landlord, this harpooneer sounds like trouble.\"\n",
      "\n",
      "\"He pays his bills,\" was all the landlord said. \"But it's getting late. You should get some rest. It's a big bed; plenty of room for two.\"\n",
      "\n",
      "Reluctantly, I followed the landlord upstairs to a cold room with a massive bed. He lit a candle, wished me good night, and disappeared.\n",
      "\n",
      "I inspected the bed and the room, finding everything to be reasonably acceptable, considering the circumstances.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"The user will provide a portion of the text of Herman Melville's \" + \\\n",
    "\"classic public domain novel Moby Dick. Your task is to translate the text into \" + \\\n",
    "\"modern American vernacular. You should try to preserve the meaning and setting \" + \\\n",
    "\"of the text while modernizing its voice. Each numbered chapter and paragraph in\" + \\\n",
    "\"the original text should have a corresponding numbered chapter or paragraph in the \" + \\\n",
    "\"translation. You are only responsible to translate the portion of the text in \" + \\\n",
    "\"the user's prompt, but you must provide a full, unabbreviated translation of \" + \\\n",
    "\"this text. If your response is interrupted, pick up in the next message exactly \" + \\\n",
    "\"where you left off, using the paragraph numbers as a point of reference.\"\n",
    "\n",
    "def prompt_llm(\n",
    "            message_list: list[dict],\n",
    "            minimum_output_tokens: int = 4096,\n",
    "            total_cumulative_cost: float = 0.0,\n",
    "            depth: int = 0,\n",
    "            max_depth: int = 16\n",
    "        ) -> tuple[list[dict], int]:\n",
    "    # Check recursion depth\n",
    "    if depth > max_depth:\n",
    "        print(\"Recursion depth limit reached.\")\n",
    "        return message_list, total_cumulative_cost\n",
    "\n",
    "    print(f\"Recursion depth: {depth}, Message list length: {len(message_list)}, Total cumulative cost: {total_cumulative_cost}\")\n",
    "\n",
    "    try:\n",
    "        # Select appropriate model based on prompt length and minimum_output_tokens\n",
    "        model = select_model(message_list, minimum_output_tokens)\n",
    "\n",
    "        # Prompt the LLM with the current message_list\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=message_list,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        # Update the message list with the response\n",
    "        message_list.append(\n",
    "                {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "            )\n",
    "        \n",
    "        # Update total cumulative cost\n",
    "        total_cumulative_cost += response.usage.prompt_tokens*model_cost[model][0] + response.usage.completion_tokens*model_cost[model][1]\n",
    "\n",
    "        print(\"Response appended to message list.\")\n",
    "\n",
    "        # If response was interrupted due to length, call function recursively with\n",
    "        # updated message_list to prompt again\n",
    "        if response.choices[0].finish_reason == \"length\":\n",
    "            print(\"Response finish reason: length. Recursing with updated message list.\")\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                minimum_output_tokens,\n",
    "                total_cumulative_cost,\n",
    "                depth + 1,\n",
    "                max_depth\n",
    "            )\n",
    "        \n",
    "        # If line number is equal to or greater than last line in text prompt, break recursion\n",
    "        output_last_line_number = get_last_line_number(response.choices[0].message.content)\n",
    "        input_last_line_number = get_last_line_number(message_list[1][\"content\"])\n",
    "        if output_last_line_number is None:\n",
    "            warnings.warn(\"Paragraph numbers missing from output text. Breaking loop.\")\n",
    "            return message_list, total_cumulative_cost\n",
    "        elif output_last_line_number >= input_last_line_number:\n",
    "            print(\"Translation complete. Returning message list.\")\n",
    "            return message_list, total_cumulative_cost\n",
    "\n",
    "    except Exception as e:\n",
    "        # Try selecting model with more conservative assumptions before raising BadRequestError\n",
    "        if isinstance(e, BadRequestError) and minimum_output_tokens < 4096:\n",
    "            print(\"BadRequestError encountered. Retrying with minimum_output_tokens=4096.\")\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                4096,\n",
    "                total_cumulative_cost,\n",
    "                depth + 1,\n",
    "                max_depth\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Exception encountered: {e}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "message_list_prompt = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": number_lines(medium_text)}\n",
    "            ]\n",
    "\n",
    "message_list_result, tokens_used = prompt_llm(message_list_prompt, 4096)\n",
    "full_output = unnumber_lines(get_full_output(message_list_result))\n",
    "print(\"Tokens used:\")\n",
    "print(tokens_used)\n",
    "print(\"Full output:\")\n",
    "print(full_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This device performs beautifullly, and we can now generate the entire translation of the first three chapters. As a bonus, we use far fewer input tokens than with the prompt-engineering approach, because we don't have to re-prompt the model for a stopping signal after each text generation step. The main problem with this approach, of course, is that it requires a one-to-one correspondence between input and output paragraph numbers, which removes some flexibility from the task performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligently chunking the input\n",
    "\n",
    "Note that up till now, we've been dealing with total context lengths no longer than about 20,000 tokens. But remember, our original context was 300,000 tokens long, and our goal was to translate the whole text. We can't submit that all at once, but we can submit it in chunks and then concatenate the responses. Since our output will be about the same length as our input, a 128,000-token context window can theoretically accommodate inputs up to about 64,000 tokens, though we'll want to leave some margin for error. We'll shoot for inputs on the order of 50,000 tokens. For continuity, we'll want to make sure that the last token of each chunk is a sentence-ending punctuation mark, perhaps ideally at the end of a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 297960.75\n",
      "Number of chunks: 6\n",
      "Chunk 1:\n",
      "CHAPTER 1. Loomings.\n",
      "Call me Ishmael. Some years ago—never mind how long precisely—having little or \n",
      "...\n",
      "and spare lines and harpoons, and spare everythings, almost, but a spare Captain and duplicate ship.\n",
      "198705\n",
      "\n",
      "Chunk 2:\n",
      "At the period of our arrival at the Island, the heaviest storage of the Pequod had been almost compl\n",
      "...\n",
      " does the passing mention of a White Friar or a White Nun, evoke such an eyeless statue in the soul?\n",
      "198563\n",
      "\n",
      "Chunk 3:\n",
      "Or what is there apart from the traditions of dungeoned warriors and kings (which will not wholly ac\n",
      "...\n",
      "r indolent crests; and across the wide trance of the sea, east nodded to west, and the sun over all.\n",
      "198372\n",
      "\n",
      "Chunk 4:\n",
      "Suddenly bubbles seemed bursting beneath my closed eyes; like vices my hands grasped the shrouds; so\n",
      "...\n",
      "ephants of antiquity often hailed the morning with their trunks uplifted in the profoundest silence.\n",
      "198415\n",
      "\n",
      "Chunk 5:\n",
      "The chance comparison in this chapter, between the whale and the elephant, so far as some aspects of\n",
      "...\n",
      "acter of the withdrawn water, the mariners readily detect any serious leakage in the precious cargo.\n",
      "198625\n",
      "\n",
      "Chunk 6:\n",
      "Now, from the South and West the Pequod was drawing nigh to Formosa and the Bashee Isles, between wh\n",
      "...\n",
      "cruising Rachel, that in her retracing search after her missing children, only found another orphan.\n",
      "199153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "# Regular expressions for finding paragraph ends, sentence-ending punctuation, and fallback to spaces\n",
    "PARAGRAPH_END = re.compile(r'\\n\\n+')\n",
    "SENTENCE_END = re.compile(r'[.!?][”’\"\\']*\\s')\n",
    "SPACE = re.compile(r'\\s')\n",
    "\n",
    "def find_nearest_break_point(search_text, start_offset):\n",
    "    \"\"\"\n",
    "    Attempts to find a split point in the search_text using a list of regular expressions.\n",
    "    Returns the offset from start_offset of the first successful match or None if no match is found.\n",
    "    \n",
    "    :param search_text: The text to search through.\n",
    "    :param start_offset: The starting offset of search_text within the larger text.\n",
    "    :return: The absolute position of the split point within the larger text or None if not found.\n",
    "    \"\"\"\n",
    "    for pattern in [PARAGRAPH_END, SENTENCE_END, SPACE]:\n",
    "        match = pattern.search(search_text)\n",
    "        if match:\n",
    "            return start_offset + match.end()\n",
    "    return None\n",
    "\n",
    "def split_into_chunks(long_text: str, max_token_length: int = 50000, margin_of_error: int = 100):\n",
    "    \"\"\"\n",
    "    Splits a given text into chunks. If the text is <= max_token_length, returns it unchunked.\n",
    "    Otherwise, splits the text into evenly sized chunks, trying not to exceed max_token_length,\n",
    "    and aiming to end each chunk at a paragraph break or a sentence-ending punctuation\n",
    "    within a margin of error.\n",
    "\n",
    "    :param long_text: The text to be split.\n",
    "    :param max_token_length: Target length of each chunk in tokens (default 50,000).\n",
    "    :param margin_of_error: Margin of error for splitting point in tokens (default 100).\n",
    "    :return: A list of text chunks.\n",
    "    \"\"\"\n",
    "    max_character_length = max_token_length * 4\n",
    "    total_characters = len(long_text)\n",
    "    margin_in_chars = margin_of_error * 4\n",
    "\n",
    "    if total_characters <= max_character_length:\n",
    "        return [long_text]\n",
    "    \n",
    "    num_chunks = math.ceil(total_characters / max_character_length)\n",
    "    optimal_char_length = total_characters // num_chunks\n",
    "\n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "\n",
    "    for chunk_index in range(num_chunks):\n",
    "        end_index = start_index + optimal_char_length\n",
    "        is_last_chunk = chunk_index == num_chunks - 1\n",
    "\n",
    "        # Adjust the end_index for the last chunk\n",
    "        if is_last_chunk:\n",
    "            end_index = total_characters\n",
    "            chunks.append(long_text[start_index:end_index].strip())\n",
    "        else:\n",
    "            search_start = max(start_index, end_index - margin_in_chars)\n",
    "            search_end = min(end_index + margin_in_chars, total_characters)\n",
    "            search_range = long_text[search_start:search_end]\n",
    "            break_point = find_nearest_break_point(search_range, search_start)\n",
    "\n",
    "            if break_point is not None:\n",
    "                end_index = break_point\n",
    "                chunks.append(long_text[start_index:end_index].strip())\n",
    "                start_index = end_index\n",
    "            else:\n",
    "                warnings.warn(f\"No suitable break point found input text for chunk {chunk_index}. Using default split.\")\n",
    "                chunks.append(long_text[start_index:end_index].strip())\n",
    "                start_index = end_index\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Print number of characters in long_text\n",
    "print(\"Number of tokens:\", len(long_text)/4)\n",
    "\n",
    "# Split the long_text into chunks\n",
    "chunks = split_into_chunks(long_text, 50000, 100)\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "\n",
    "# Print first and last 100 characters in each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}:\")\n",
    "    print(chunk[:100])\n",
    "    print(\"...\")\n",
    "    print(chunk[-100:])\n",
    "    print(len(chunk))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Let's put all these techniques together to see if the model now performs the task. Since querying these models is expensive, we'll use 20,000-word chunks and only submit the first chunk. The rest is left as an exercise for the reader.\n",
    "\n",
    "### Adjustable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output file paths\n",
    "input_filepath = \"sample_input.txt\"\n",
    "output_filepath = \"sample_output.txt\"\n",
    "\n",
    "# Select models to use based on token count\n",
    "models = {\n",
    "        16384: \"gpt-3.5-turbo\",\n",
    "        128000: \"gpt-4-turbo-preview\"\n",
    "    }\n",
    "\n",
    "# Set model costs per input and output token for calculating total cumulative spend\n",
    "model_cost = {\n",
    "    \"gpt-3.5-turbo\": (0.0005/1000, 0.0015/1000),\n",
    "    \"gpt-4\": (0.03/1000, 0.06/1000),\n",
    "    \"gpt-4-32k\": (0.06/1000, 0.12/1000),\n",
    "    \"gpt-4-turbo-preview\": (0.01/1000, 0.03/1000)\n",
    "}\n",
    "\n",
    "# Set system prompt for the Moby Dick translation task\n",
    "system_prompt = \"The user will provide a portion of the text of Herman Melville's \" + \\\n",
    "\"classic public domain novel Moby Dick. Your task is to translate the text into \" + \\\n",
    "\"modern American vernacular. You should try to preserve the meaning and setting \" + \\\n",
    "\"of the text while modernizing its voice. Each numbered paragraph in the original \" + \\\n",
    "\"text should have a corresponding numbered paragraph in the translation. (When \" + \\\n",
    "\"numbering, don't treat chapter headers as paragraphs.) You are only \" + \\\n",
    "\"responsible to translate the portion of the text in the user's prompt, but you \" + \\\n",
    "\"must provide a full, unabbreviated translation of this text. If your response is \" + \\\n",
    "\"interrupted, pick up in the next message exactly where you left off, using the \" + \\\n",
    "\"paragraph numbers as a point of reference. If you complete the full task, \" + \\\n",
    "\"I will tip you $20.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import ChatCompletion\n",
    "from openai import BadRequestError\n",
    "\n",
    "client = OpenAI(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "# Regular expressions for finding paragraph ends, sentence-ending punctuation, and fallback to spaces\n",
    "PARAGRAPH_END = re.compile(r'\\n\\n+')\n",
    "SENTENCE_END = re.compile(r'[.!?][”’\"\\']*\\s')\n",
    "SPACE = re.compile(r'\\s')\n",
    "\n",
    "# Regular expression pattern for chapter titles\n",
    "CHAPTER_TITLE = re.compile(r\"^CHAPTER \\d+\\..*$\", re.IGNORECASE)\n",
    "\n",
    "def find_nearest_break_point(search_text, start_offset):\n",
    "    \"\"\"\n",
    "    Attempts to find a split point in the search_text using a list of regular expressions.\n",
    "    Returns the offset from start_offset of the first successful match or None if no match is found.\n",
    "    \n",
    "    :param search_text: The text to search through.\n",
    "    :param start_offset: The starting offset of search_text within the larger text.\n",
    "    :return: The absolute position of the split point within the larger text or None if not found.\n",
    "    \"\"\"\n",
    "    for pattern in [PARAGRAPH_END, SENTENCE_END, SPACE]:\n",
    "        match = pattern.search(search_text)\n",
    "        if match:\n",
    "            return start_offset + match.end()\n",
    "    return None\n",
    "\n",
    "\n",
    "def split_into_chunks(long_text: str, max_token_length: int = 50000, margin_of_error: int = 100):\n",
    "    \"\"\"\n",
    "    Splits a given text into chunks. If the text is <= max_token_length, returns it unchunked.\n",
    "    Otherwise, splits the text into evenly sized chunks, trying not to exceed max_token_length,\n",
    "    and aiming to end each chunk at a paragraph break or a sentence-ending punctuation\n",
    "    within a margin of error.\n",
    "\n",
    "    :param long_text: The text to be split.\n",
    "    :param max_token_length: Target length of each chunk in tokens (default 50,000).\n",
    "    :param margin_of_error: Margin of error for splitting point in tokens (default 100).\n",
    "    :return: A list of text chunks.\n",
    "    \"\"\"\n",
    "    max_character_length = max_token_length * 4\n",
    "    total_characters = len(long_text)\n",
    "    margin_in_chars = margin_of_error * 4\n",
    "\n",
    "    if total_characters <= max_character_length:\n",
    "        return [long_text]\n",
    "    \n",
    "    num_chunks = math.ceil(total_characters / max_character_length)\n",
    "    optimal_char_length = total_characters // num_chunks\n",
    "\n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "\n",
    "    for chunk_index in range(num_chunks):\n",
    "        end_index = start_index + optimal_char_length\n",
    "        is_last_chunk = chunk_index == num_chunks - 1\n",
    "\n",
    "        # Adjust the end_index for the last chunk\n",
    "        if is_last_chunk:\n",
    "            end_index = total_characters\n",
    "            chunks.append(long_text[start_index:end_index].strip())\n",
    "        else:\n",
    "            search_start = max(start_index, end_index - margin_in_chars)\n",
    "            search_end = min(end_index + margin_in_chars, total_characters)\n",
    "            search_range = long_text[search_start:search_end]\n",
    "            break_point = find_nearest_break_point(search_range, search_start)\n",
    "\n",
    "            if break_point is not None:\n",
    "                end_index = break_point\n",
    "                chunks.append(long_text[start_index:end_index].strip())\n",
    "                start_index = end_index\n",
    "            else:\n",
    "                warnings.warn(f\"No suitable break point found input text for chunk {chunk_index}. Using default split.\")\n",
    "                chunks.append(long_text[start_index:end_index].strip())\n",
    "                start_index = end_index\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def estimate_context_length(context: str) -> int:\n",
    "    # Count characters in the input\n",
    "    prompt_length = len(context)\n",
    "\n",
    "    # Assume ~4 characters per token\n",
    "    prompt_token_count = prompt_length/4\n",
    "\n",
    "    return int(prompt_token_count)\n",
    "\n",
    "\n",
    "def select_model(message_list: list[dict], minimum_output_tokens: int = 4096) -> str:\n",
    "    # Validate minimum_output_tokens\n",
    "    if minimum_output_tokens < 1 or minimum_output_tokens > 4096:\n",
    "        raise ValueError(\"minimum_output_tokens must be between 1 and 4096.\")\n",
    "\n",
    "    # Estimate the context length including the minimum output tokens\n",
    "    input_tokens = estimate_context_length(str(message_list))\n",
    "    total_tokens = input_tokens + minimum_output_tokens\n",
    "\n",
    "    # Filter models that can handle the total_tokens and select the smallest one\n",
    "    suitable_models = [size for size in models.keys() if size >= total_tokens]\n",
    "\n",
    "    if not suitable_models:\n",
    "        # If no model supports the context length, raise an error\n",
    "        # (Note: This constraint *should* prevent RateLimitError and BadRequestError.)\n",
    "        raise ValueError(\"Context length exceeds the maximum supported token count.\")\n",
    "\n",
    "    selected_model_size = min(suitable_models)\n",
    "    return models[selected_model_size]\n",
    "\n",
    "\n",
    "def get_full_output(message_list):\n",
    "    # Join content with a space, ensuring 'content' key exists and is not empty\n",
    "    return \"\\n\".join([message[\"content\"] for message in message_list if message.get(\"role\") == \"assistant\" and message.get(\"content\", \"\")])\n",
    "\n",
    "\n",
    "def number_lines(text: str) -> str:\n",
    "    # Replace double line breaks with single line breaks\n",
    "    text = text.replace(\"\\n\\n\", \"\\n\")\n",
    "    # Split the text by line breaks\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    # Initialize a paragraph list and paragraph counter\n",
    "    numbered_paragraphs = []\n",
    "    paragraph_number = 1\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # Add a number if the paragraph is a chapter title and increment paragraph number\n",
    "        if CHAPTER_TITLE.match(paragraph):\n",
    "            numbered_paragraphs.append(paragraph)\n",
    "        else:\n",
    "            numbered_paragraphs.append(f\"{paragraph_number} {paragraph}\")\n",
    "            paragraph_number += 1\n",
    "\n",
    "    # Join the paragraphs back together\n",
    "    return \"\\n\".join(numbered_paragraphs)\n",
    "\n",
    "\n",
    "def unnumber_lines(text: str) -> str:\n",
    "    # Replace any double line breaks with single line breaks\n",
    "    text = text.replace(\"\\n\\n\", \"\\n\")\n",
    "    # Split the text by line breaks\n",
    "    paragraphs = text.strip().split(\"\\n\")\n",
    "    # Remove the paragraph numbers\n",
    "    unnumbered_paragraphs = [re.sub(r\"^\\d+ \", \"\", paragraph) for paragraph in paragraphs]\n",
    "    # Join the paragraphs back together\n",
    "    return \"\\n\\n\".join(unnumbered_paragraphs)\n",
    "\n",
    "\n",
    "def get_last_line_number(text: str) -> int:\n",
    "    # Split the text by line breaks\n",
    "    paragraphs = text.strip().split(\"\\n\")\n",
    "    \n",
    "    # Get the last paragraph number\n",
    "    try:\n",
    "        print(paragraphs[-1])\n",
    "        return int(paragraphs[-1].split(\" \")[0])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def prompt_llm(\n",
    "            message_list: list[dict],\n",
    "            minimum_output_tokens: int = 4096,\n",
    "            total_cumulative_cost: float = 0.0,\n",
    "            depth: int = 0,\n",
    "            max_depth: int = 16\n",
    "        ) -> tuple[list[dict], int]:\n",
    "    # Check recursion depth\n",
    "    if depth > max_depth:\n",
    "        print(\"Recursion depth limit reached.\")\n",
    "        return message_list, total_cumulative_cost\n",
    "\n",
    "    print(f\"Recursion depth: {depth}, Message list length: {len(message_list)}, Total cumulative cost: {total_cumulative_cost}\")\n",
    "\n",
    "    try:\n",
    "        # Select appropriate model based on prompt length and minimum_output_tokens\n",
    "        model = select_model(message_list, minimum_output_tokens)\n",
    "\n",
    "        # Prompt the LLM with the current message_list\n",
    "        response: ChatCompletion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=message_list,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        # Update the message list with the response\n",
    "        message_list.append(\n",
    "                {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "            )\n",
    "        \n",
    "        # Update total cumulative cost\n",
    "        total_cumulative_cost += response.usage.prompt_tokens*model_cost[model][0] + response.usage.completion_tokens*model_cost[model][1]\n",
    "\n",
    "        print(\"Response appended to message list.\")\n",
    "\n",
    "        # If response was interrupted due to length, call function recursively with\n",
    "        # updated message_list to prompt again\n",
    "        if response.choices[0].finish_reason == \"length\":\n",
    "            print(\"Response finish reason: length. Recursing with updated message list.\")\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                minimum_output_tokens,\n",
    "                total_cumulative_cost,\n",
    "                depth + 1,\n",
    "                max_depth\n",
    "            )\n",
    "        \n",
    "        # If line number isn't equal to or greater than last line in text prompt, call\n",
    "        # function recursively with message_list to prompt again\n",
    "        print(\"Last output line: \")\n",
    "        output_last_line_number = get_last_line_number(response.choices[0].message.content)\n",
    "        print(\"Last input line: \")\n",
    "        input_last_line_number = get_last_line_number(message_list[1][\"content\"])\n",
    "        if output_last_line_number is None:\n",
    "            warnings.warn(\"Paragraph numbers missing from output text. Breaking loop.\")\n",
    "            # Drop the last message from the message list\n",
    "            message_list.pop()\n",
    "\n",
    "            return message_list, total_cumulative_cost\n",
    "        elif output_last_line_number >= input_last_line_number:\n",
    "            print(\"Translation complete. Returning message list.\")\n",
    "            return message_list, total_cumulative_cost\n",
    "        else:\n",
    "            print(\"Translation incomplete. Recursing with updated message list.\")\n",
    "            user_prompt = \"The original prompt ended with paragraph \" + \\\n",
    "            str(input_last_line_number) + \", and you have only modernized through \" + \\\n",
    "            \"paragraph \" + str(output_last_line_number) + \", so your task is not \" + \\\n",
    "            \"yet complete. To earn your $20 tip, you must resume from paragraph \" + \\\n",
    "            str(output_last_line_number + 1) + \", remembering to number paragraphs \" + \\\n",
    "            \"and to maintain a one-to-one correspondence between input and output \" + \\\n",
    "            \"paragraph numbers until the task is complete.\"\n",
    "            message_list.append(\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            )\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                minimum_output_tokens,\n",
    "                total_cumulative_cost,\n",
    "                depth + 1,\n",
    "                max_depth\n",
    "            )            \n",
    "\n",
    "    except Exception as e:\n",
    "        # Try selecting model with more conservative assumptions before raising BadRequestError\n",
    "        if isinstance(e, BadRequestError) and minimum_output_tokens < 4096:\n",
    "            print(\"BadRequestError encountered. Retrying with minimum_output_tokens=4096.\")\n",
    "            return prompt_llm(\n",
    "                message_list,\n",
    "                4096,\n",
    "                total_cumulative_cost,\n",
    "                depth + 1,\n",
    "                max_depth\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Exception encountered: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated token count of input text: 297792\n",
      "Chunk 1:\n",
      "CHAPTER 1. Loomings.\n",
      "Call me Ishmael. Some years ago—never mind how long precisely—having little or \n",
      "...\n",
      "self-containing stronghold—a lofty Ehrenbreitstein, with a perennial well of water within the walls.\n",
      "Chunk length: ~19900 tokens\n",
      "\n",
      "Chunk 2:\n",
      "But the side ladder was not the only strange feature of the place, borrowed from the chaplain’s form\n",
      "...\n",
      "on it, not to speak of my three years’ beef and board, for which I would not have to pay one stiver.\n",
      "Chunk length: ~19900 tokens\n",
      "\n",
      "Chunk 3:\n",
      "It might be thought that this was a poor way to accumulate a princely fortune—and so it was, a very \n",
      "...\n",
      " themselves there, about something which he would find out when he obeyed the order, and not sooner.\n",
      "Chunk length: ~20000 tokens\n",
      "\n",
      "Chunk 4:\n",
      "What, perhaps, with other things, made Stubb such an easy-going, unfearing man, so cheerily trudging\n",
      "...\n",
      "stars; even as the look-outs of a modern ship sing out for a sail, or a whale just bearing in sight.\n",
      "Chunk length: ~19800 tokens\n",
      "\n",
      "Chunk 5:\n",
      "In Saint Stylites, the famous Christian hermit of old times, who built him a lofty stone pillar in t\n",
      "...\n",
      " does the passing mention of a White Friar or a White Nun, evoke such an eyeless statue in the soul?\n",
      "Chunk length: ~19800 tokens\n",
      "\n",
      "Chunk 6:\n",
      "Or what is there apart from the traditions of dungeoned warriors and kings (which will not wholly ac\n",
      "...\n",
      "h draft of my will. “Queequeg,” said I, “come along, you shall be my lawyer, executor, and legatee.”\n",
      "Chunk length: ~19900 tokens\n",
      "\n",
      "Chunk 7:\n",
      "It may seem strange that of all men sailors should be tinkering at their last wills and testaments, \n",
      "...\n",
      "w some five feet long. Ah, my gallant captain, why did ye not give us Jonah looking out of that eye!\n",
      "Chunk length: ~19800 tokens\n",
      "\n",
      "Chunk 8:\n",
      "Nor are the most conscientious compilations of Natural History for the benefit of the young and tend\n",
      "...\n",
      " of the sharks,* by striking the keen steel deep into their skulls, seemingly their only vital part.\n",
      "Chunk length: ~19800 tokens\n",
      "\n",
      "Chunk 9:\n",
      "But in the foamy confusion of their mixed and struggling hosts, the marksmen could not always hit th\n",
      "...\n",
      "t out upon the overhanging mainyard-arm, to the part where it exactly projects over the hoisted Tun.\n",
      "Chunk length: ~19800 tokens\n",
      "\n",
      "Chunk 10:\n",
      "He has carried with him a light tackle called a whip, consisting of only two parts, travelling throu\n",
      "...\n",
      "ephants of antiquity often hailed the morning with their trunks uplifted in the profoundest silence.\n",
      "Chunk length: ~19900 tokens\n",
      "\n",
      "Chunk 11:\n",
      "The chance comparison in this chapter, between the whale and the elephant, so far as some aspects of\n",
      "...\n",
      " but from that hour the little negro went about the deck an idiot; such, at least, they said he was.\n",
      "Chunk length: ~19800 tokens\n",
      "\n",
      "Chunk 12:\n",
      "The sea had jeeringly kept his finite body up, but drowned the infinite of his soul. Not drowned ent\n",
      "...\n",
      "ew disordered joints; and in place of the weighty and majestic, but boneless flukes, an utter blank!\n",
      "Chunk length: ~19900 tokens\n",
      "\n",
      "Chunk 13:\n",
      "How vain and foolish, then, thought I, for timid untravelled man to try to comprehend aright this wo\n",
      "...\n",
      "ings to come—their two captains in themselves impersonated the whole striking contrast of the scene.\n",
      "Chunk length: ~19800 tokens\n",
      "\n",
      "Chunk 14:\n",
      "“Come aboard, come aboard!” cried the gay Bachelor’s commander, lifting a glass and a bottle in the \n",
      "...\n",
      "and attaching one to his basket prepared a pin for the other end, in order to fasten it at the rail.\n",
      "Chunk length: ~19800 tokens\n",
      "\n",
      "Chunk 15:\n",
      "This done, with that end yet in his hand and standing beside the pin, he looked round upon his crew,\n",
      "...\n",
      "cruising Rachel, that in her retracing search after her missing children, only found another orphan.\n",
      "Chunk length: ~20100 tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(input_filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "    long_text = file.read()\n",
    "\n",
    "print(\"Estimated token count of input text: \" + str(estimate_context_length(long_text)))\n",
    "\n",
    "# Split the long_text into chunks\n",
    "chunks = split_into_chunks(long_text, 20000, 100)\n",
    "\n",
    "# Inspect chunks by printing first and last 100 characters in each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}:\")\n",
    "    print(chunk[:100])\n",
    "    print(\"...\")\n",
    "    print(chunk[-100:])\n",
    "    print(f\"Chunk length: ~{int(round(len(chunk)/4,-2))} tokens\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursion depth: 0, Message list length: 2, Total cumulative cost: 0.0\n",
      "Response appended to message list.\n",
      "Response finish reason: length. Recursing with updated message list.\n",
      "Recursion depth: 1, Message list length: 3, Total cumulative cost: 0.31349000000000005\n",
      "Response appended to message list.\n",
      "Last output line: \n",
      "111 The chapel service, a moment of communal reflection and spiritual preparation, provided a foundation of strength and resolve for the challenges and adventures that lay ahead, a reminder of the enduring human spirit and the bonds that unite us in our shared journey through life.\n",
      "Last input line: \n",
      "140 I pondered some time without fully comprehending the reason for this. Father Mapple enjoyed such a wide reputation for sincerity and sanctity, that I could not suspect him of courting notoriety by any mere tricks of the stage. No, thought I, there must be some sober reason for this thing; furthermore, it must symbolize something unseen. Can it be, then, that by that act of physical isolation, he signifies his spiritual withdrawal for the time, from all outward worldly ties and connexions? Yes, for replenished with the meat and wine of the word, to the faithful man of God, this pulpit, I see, is a self-containing stronghold—a lofty Ehrenbreitstein, with a perennial well of water within the walls.\n",
      "Translation incomplete. Recursing with updated message list.\n",
      "Recursion depth: 2, Message list length: 5, Total cumulative cost: 0.5840400000000001\n",
      "Response appended to message list.\n",
      "Last output line: \n",
      "140 Watching Father Mapple prepare to speak, I was struck by the thoughtfulness behind the design of the pulpit and its significance. It wasn't just about practicality; it was a profound statement of faith, a physical representation of the chaplain's spiritual withdrawal from the world to commune with the divine.\n",
      "Last input line: \n",
      "140 I pondered some time without fully comprehending the reason for this. Father Mapple enjoyed such a wide reputation for sincerity and sanctity, that I could not suspect him of courting notoriety by any mere tricks of the stage. No, thought I, there must be some sober reason for this thing; furthermore, it must symbolize something unseen. Can it be, then, that by that act of physical isolation, he signifies his spiritual withdrawal for the time, from all outward worldly ties and connexions? Yes, for replenished with the meat and wine of the word, to the faithful man of God, this pulpit, I see, is a self-containing stronghold—a lofty Ehrenbreitstein, with a perennial well of water within the walls.\n",
      "Translation complete. Returning message list.\n",
      "Chunk 1 cost: 0.8830500000000001\n",
      "Cumulative cost:\n",
      "0.8830500000000001\n"
     ]
    }
   ],
   "source": [
    "full_output = \"\"\n",
    "cumulative_cost = 0.0\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Debug: Only process first chunk\n",
    "    if i == 0:\n",
    "        message_list_prompt = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": number_lines(chunk)}\n",
    "                ]\n",
    "\n",
    "        message_list_result, cost = prompt_llm(message_list_prompt, 4096)\n",
    "        full_output += unnumber_lines(get_full_output(message_list_result)) + \"\\n\"\n",
    "        cumulative_cost += cost\n",
    "\n",
    "        print(f\"Chunk {i + 1} cost: {cost}\")\n",
    "\n",
    "print(\"Cumulative cost:\")\n",
    "print(cumulative_cost)\n",
    "\n",
    "with open(\"sample_output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(full_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took some wrestling, but we finally got the expected output, which can be inspected in the `sample_output.txt` file.\n",
    "\n",
    "For reasons of cost saving, I am not going to process the entire text. However, to do so, you can simply remove `if i == 0:` from the `for` loop in the `Execution` section and unindent the following code block. (You can also feel free to remove debugging print statements throughout the code!)\n",
    "\n",
    "Note that we've got 13 chunks, and the first chunk cost us nearly a dollar to process, which suggests we could expect to spend $10-12 to process the entire text of Moby Dick. Costs would increase if we used a larger chunk size, such as 50,000 tokens instead of 20,000. (I won't tell you how much I spent on API token usage for experimentation while developing this notebook!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-developer-playground-6LGp-str",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
