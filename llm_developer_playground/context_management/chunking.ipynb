{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-form text editing with chunking, summarization, and iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model limitations\n",
    "\n",
    "Let's first define some constants with descriptive statistics about OpenAI models and their capabilities. These are the constraints that we will be working with as we try to define an LLM pipeline to modernize Herman Melville's very lengthy novel *Moby Dick*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI models and their current costs\n",
    "LLM_MODELS = [\n",
    "    {\n",
    "        \"name\": \"gpt-3.5-turbo\",\n",
    "        \"max_tokens\": 16385,\n",
    "        \"prompt_cost_per_token\": 0.5 / 1000000,\n",
    "        \"completion_cost_per_token\": 1.5 / 1000000,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gpt-4-turbo\",\n",
    "        \"max_tokens\": 128000,\n",
    "        \"prompt_cost_per_token\": 10 / 1000000,\n",
    "        \"completion_cost_per_token\": 30 / 1000000,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gpt-4\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"prompt_cost_per_token\": 30 / 1000000,\n",
    "        \"completion_cost_per_token\": 60 / 1000000,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gpt-4-32k\",\n",
    "        \"max_tokens\": 32768,\n",
    "        \"prompt_cost_per_token\": 30 / 1000000,\n",
    "        \"completion_cost_per_token\": 0.12 / 1000000,\n",
    "    }\n",
    "]\n",
    "\n",
    "# OpenAI currently limits output tokens to 4096 for all models\n",
    "MAX_OUTPUT_TOKENS = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task\n",
    "\n",
    "The goal of our exercise will be to modernize the text of Herman Melville's classic public domain novel Moby Dick. Since the book is much too long for even long-context (128,000-token) LLMs, much less the 4096-token output limit, we will use a combination of chunking, summarization, synthesis, and multi-turn text generation with a sliding context window to maintain consistency across the task.\n",
    "\n",
    "We'll begin by loading the text of Moby Dick, which is downloaded from Project Gutenberg and provided in this repository as `sample_input.txt`. Using the heuristic of about 4 English characters per token, we can estimate the length of the text at close to 300,000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated token count: 297792\n"
     ]
    }
   ],
   "source": [
    "# Lambda function to estimate number of tokens based on 4 characters/token\n",
    "estimate_tokens = lambda str: round(len(str)/4)\n",
    "\n",
    "# Open sample input file, get the text, and estimate number of tokens\n",
    "with open(\"sample_input.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"Estimated token count: \" + str(estimate_tokens(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Next, let's define some logic to chunk the text. We'll set a max_length for each chunk and then find the last sentence ending before that length (using a reasonable estimate of the ratio of English characters to tokens as a heuristic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: CHAPTER 1. Loomings.\n",
      "Call me Ishmael. Some years a...\n",
      "Estimated token count: 101272\n",
      "\n",
      "Chunk 2: CHAPTER 43. Hark!\n",
      "“HIST! Did you hear that noise, ...\n",
      "Estimated token count: 97692\n",
      "\n",
      "Chunk 3: CHAPTER 87. The Grand Armada.\n",
      "The long and narrow ...\n",
      "Estimated token count: 98827\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from warnings import warn\n",
    "from math import ceil\n",
    "\n",
    "# Regular expressions for split points\n",
    "SPLIT_PATTERNS = [\n",
    "    re.compile(r'\\s'), # WHITESPACE\n",
    "    re.compile(r'[.!?][”’\"\\']*\\s'), # SENTENCE_END\n",
    "    re.compile(r'\\n\\n+'), # PARAGRAPH END\n",
    "    re.compile(r'\\n\\n+(?=CHAPTER \\d)', re.IGNORECASE), # CHAPTER END\n",
    "]\n",
    "\n",
    "# Find the last occurrence of a pattern in a text chunk\n",
    "def find_last(text: str, pattern: str) -> int or None:\n",
    "    # Get a list of matches for the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text))\n",
    "    try:\n",
    "        # Get the last one's end position\n",
    "        split_point = matches[-1].end()\n",
    "    except:\n",
    "        split_point = None\n",
    "\n",
    "    return split_point\n",
    "\n",
    "\n",
    "# Split text into evenly sized chunks shorter than max_characters\n",
    "def split_text(\n",
    "            text: str,\n",
    "            max_characters: int | float,\n",
    "            split_patterns: list[re.Pattern] = SPLIT_PATTERNS\n",
    "        ) -> list[str]:\n",
    "    # Cooerce max_characters to an integer\n",
    "    max_characters = int(max_characters)\n",
    "    text_length = len(text)\n",
    "    \n",
    "    # Ceiling divide the text length by the max_length to get the number of chunks\n",
    "    num_chunks = ceil(text_length / max_characters)\n",
    "\n",
    "    # Ceiling divide the text length by the number of chunks to get the target_length\n",
    "    target_length = ceil(text_length / num_chunks)\n",
    "\n",
    "    # Get target split indices for the text\n",
    "    target_split_indices = [target_length * i for i in range(1, num_chunks)]\n",
    "\n",
    "    # Initialize split_indices, threshold, and patterns\n",
    "    threshold = max(max_characters - target_length, 500)\n",
    "    split_indices = [None] * (num_chunks - 1)\n",
    "    patterns = split_patterns.copy()\n",
    "\n",
    "    # Try patterns until each split index - corresponding target split index <= threshold\n",
    "    while not all(split_indices) and patterns:\n",
    "        # Pop the last split_pattern from the list and get all match end positions\n",
    "        pattern = patterns.pop()\n",
    "        end_positions = [match.end() for match in re.finditer(pattern, text)]\n",
    "\n",
    "        # For each None index in split_indices, find the nearest end_position (by absolute difference)\n",
    "        for i, split_index in enumerate(split_indices):\n",
    "            if split_index is None:\n",
    "                nearest_end_position = min(end_positions, key=lambda x: abs(x - target_split_indices[i]))\n",
    "                if abs(nearest_end_position - target_split_indices[i]) <= threshold:\n",
    "                    split_indices[i] = nearest_end_position\n",
    "    \n",
    "    # If not all split_indices are found, raise a warning and use the target_split_indices\n",
    "    if not all(split_indices):\n",
    "        warn(\"Could not split text evenly. Using target split indices instead.\")\n",
    "        split_indices = target_split_indices\n",
    "\n",
    "    # Split the text into chunks using the split_indices\n",
    "    chunks = [text[i:j] for i, j in zip([0] + split_indices, split_indices + [None])]\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Get model object for GPT-4 Turbo\n",
    "model = [model for model in LLM_MODELS if model[\"name\"] == \"gpt-4-turbo\"][0]\n",
    "\n",
    "# Allow input no longer than gpt-4-turbo max_tokens - MAX_OUTPUT_TOKENS - 1000\n",
    "estimate_chars = lambda tokens: tokens * 4\n",
    "max_input_length = estimate_chars(model[\"max_tokens\"] - MAX_OUTPUT_TOKENS - 1000)\n",
    "\n",
    "# Split the input text into chunks no longer than max_input_length\n",
    "input_chunks = split_text(text, max_input_length)\n",
    "\n",
    "# Estimate the number of tokens for each chunk\n",
    "token_counts = [estimate_tokens(chunk) for chunk in input_chunks]\n",
    "\n",
    "# Print estimated token count and first 50 characters of each chunk\n",
    "for i, chunk in enumerate(input_chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk[:50]}...\")\n",
    "    print(f\"Estimated token count: {token_counts[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous summarization and summary synthesis over multiple chunks\n",
    "\n",
    "We'll use GPT-4-Turbo with an async client to summarize several large text chunks simultaneously, and then we'll synthesize the chunk summaries into a final summary. This lets us complete the task in constant time (O(1) time complexity), regardless of the length of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<coroutine object AsyncCompletions.create at 0x0000019C19318580>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Optional\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "from openai.types.chat import ChatCompletion\n",
    "from openai import BadRequestError\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "\n",
    "# Find smallest model that accommodates message list input + MAX_OUTPUT_TOKENS response\n",
    "def select_model(message_list: list[dict]) -> str:\n",
    "    # Estimate the context length including the longest possible output plus a buffer\n",
    "    estimated_tokens = estimate_tokens(str(message_list)) + MAX_OUTPUT_TOKENS + 1000\n",
    "\n",
    "    # Filter models that can handle the total_tokens and select the smallest one\n",
    "    best_model = None\n",
    "    for model in LLM_MODELS:\n",
    "      max_tokens = model[\"max_tokens\"]\n",
    "      if model and max_tokens > estimated_tokens and max_tokens < best_model[\"max_tokens\"]:\n",
    "          best_model = model\n",
    "    \n",
    "    # If we found a model, return its name; otherwise, raise an error\n",
    "    if best_model:\n",
    "      return best_model[\"name\"]\n",
    "    raise ValueError(\"Context length exceeds the maximum supported token count.\")\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constant for maximum number of API calls\n",
    "MAX_API_CALLS = 5\n",
    "\n",
    "# Variable to track the number of API calls\n",
    "api_calls_counter = 0\n",
    "\n",
    "async def make_api_call(model: str, messages: list[dict]) -> Optional[ChatCompletion]:\n",
    "    global api_calls_counter\n",
    "\n",
    "    # Check if the maximum number of API calls is reached\n",
    "    if api_calls_counter >= MAX_API_CALLS:\n",
    "        logger.warning(\"Maximum number of API calls reached. Skipping API call.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Make the API call with a timeout\n",
    "        response = await asyncio.wait_for(\n",
    "            async_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0\n",
    "            ),\n",
    "            timeout=100.0  # Adjust the timeout as needed\n",
    "        )\n",
    "\n",
    "        # Increment the API calls counter\n",
    "        api_calls_counter += 1\n",
    "\n",
    "        return response\n",
    "\n",
    "    except asyncio.TimeoutError:\n",
    "        logger.error(\"API call timed out.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"API call failed: {str(e)}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Initialize sync OpenAI client\n",
    "sync_client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    organization=os.getenv(\"OPENAI_ORGANIZATION\")\n",
    ")\n",
    "\n",
    "# Instructions for the LLM synthesizer\n",
    "SYNTHESIS_INSTRUCTIONS = (\n",
    "    \"Synthesize the summaries of large text chunks from Herman Melville's Moby Dick \"\n",
    "    \"into a single cohesive summary. This summary should serve as a comprehensive \"\n",
    "    \"reference cheat sheet for an editor tasked with modernizing the text. Focus on \"\n",
    "    \"integrating key plot points, character arcs, and thematic elements to provide \"\n",
    "    \"a useful guide for text modernization.\"\n",
    ")\n",
    "\n",
    "# Function to call an LLM to synthesize multiple summaries\n",
    "def synthesize_summaries_with_llm(chunk_summaries: list[str]) -> str:\n",
    "    # Concatenate chunk summaries into a single string with informative separators\n",
    "    chunk_separator = \"Chunk:\\n\\n\"\n",
    "    text = chunk_separator + chunk_separator.join(chunk_summaries)\n",
    "\n",
    "    # Select the smallest model that can handle the input + output tokens\n",
    "    system_message = {\"role\": \"system\", \"content\": SYNTHESIS_INSTRUCTIONS}\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    model_name = select_model([system_message, user_message])\n",
    "\n",
    "    # Call the OpenAI API to synthesize the summaries\n",
    "    response = sync_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[user_message, system_message],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # Extract the response text and return it\n",
    "    model = [model for model in LLM_MODELS if model[\"name\"] == model_name][0]\n",
    "    return {\n",
    "        \"text\": response.choices[0].message.content,\n",
    "        \"llm_models\": [model_name],\n",
    "        \"llm_cost\": response.usage.prompt_tokens*model[\"prompt_cost_per_token\"] + \\\n",
    "            response.usage.completion_tokens*model[\"completion_cost_per_token\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize async OpenAI client\n",
    "async_client = AsyncOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    organization=os.getenv(\"OPENAI_ORGANIZATION\")\n",
    ")\n",
    "\n",
    "# Instructions for the LLM summarizer\n",
    "SUMMARY_INSTRUCTIONS = \"The user will provide a portion of the text of Herman \" + \\\n",
    "\"Melville's classic public domain novel Moby Dick. Your task is to summarize the \" + \\\n",
    "\"plot and characters in a useful cheat sheet intended for use by an editor who \" + \\\n",
    "\"will be modernizing the text. You should only summarize the portion of the text \" + \\\n",
    "\"in the user's prompt.\"\n",
    "\n",
    "# Pydantic model for our result\n",
    "class Result(BaseModel):\n",
    "    text: str\n",
    "    llm_models: list[str]\n",
    "    llm_cost: float\n",
    "\n",
    "async def summarize_with_llm(input_chunks: list[str], model: dict) -> Result:\n",
    "    # Initialize variables to track total cost and results\n",
    "    total_cumulative_cost = 0\n",
    "    tasks = []\n",
    "    system_message = {\"role\": \"system\", \"content\": SUMMARY_INSTRUCTIONS}\n",
    "\n",
    "    # Prepare the tasks for asynchronous API calls\n",
    "    for chunk in input_chunks:\n",
    "        # Add chunk as user message\n",
    "        user_message = {\"role\": \"user\", \"content\": chunk}\n",
    "\n",
    "        # Prepare the asynchronous task\n",
    "        task = async_client.chat.completions.create(\n",
    "            model=model[\"name\"],\n",
    "            messages=[user_message, system_message],\n",
    "            temperature=0\n",
    "        )\n",
    "        tasks.append(task)\n",
    "\n",
    "    # Execute the asynchronous API calls and process responses\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    chunk_summaries = []\n",
    "    for response in responses:\n",
    "        # Access the response text, strip whitespace, and concatenate\n",
    "        chunk_summaries.append(response.choices[0].message.content.strip())\n",
    "\n",
    "        # Update total cumulative cost\n",
    "        total_cumulative_cost += response.usage.prompt_tokens*model[\"prompt_cost_per_token\"] + \\\n",
    "            response.usage.completion_tokens*model[\"completion_cost_per_token\"]\n",
    "\n",
    "    # Synthesize the chunk summaries into a single summary\n",
    "    if len(chunk_summaries) > 1:\n",
    "        synthesis_result: Result = synthesize_summaries_with_llm(chunk_summaries)\n",
    "\n",
    "        return Result(\n",
    "                text=synthesis_result[\"text\"],\n",
    "                llm_models=[model[\"name\"]].extend(synthesis_result[\"llm_models\"]),\n",
    "                llm_cost=total_cumulative_cost + synthesis_result[\"llm_cost\"]\n",
    "            )\n",
    "    else:\n",
    "        return Result(\n",
    "                text=chunk_summaries[0],\n",
    "                llm_models=[model[\"name\"]],\n",
    "                llm_cost=total_cumulative_cost\n",
    "            )\n",
    "\n",
    "\n",
    "# Get model object for GPT-4 Turbo\n",
    "model = [model for model in LLM_MODELS if model[\"name\"] == \"gpt-4-turbo\"][0]\n",
    "\n",
    "# Call the summarizer function with the sample input text\n",
    "result = await summarize_with_llm(text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_INSTRUCTIONS = \"The user will provide a portion of the text of Herman \" + \\\n",
    "\"Melville's classic public domain novel Moby Dick. Your task is to render the \" + \\\n",
    "\"text in modern American vernacular. You should try to preserve the meaning and \" + \\\n",
    "\"setting of the text, while modernizing its voice. You are only responsible to \" + \\\n",
    "\"render the portion of the text in the user's prompt, but you must provide a \" + \\\n",
    "\"full, unabbreviated rendition of this text.\"\n",
    "\n",
    "def edit_transcription_with_llm(llm_input: str) -> str:\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        organization=os.getenv(\"OPENAI_ORGANIZATION\")\n",
    "    )\n",
    "\n",
    "    # Split the input into chunks if it exceeds 4/5 of OpenAI's output token limit (converted to characters)\n",
    "    max_input_length = (MAX_OUTPUT_TOKENS*4)*4//5\n",
    "    input_chunks = split_text(llm_input, max_input_length)\n",
    "\n",
    "    total_cumulative_cost = 0\n",
    "    message_list = [{\"role\": \"system\", \"content\": LLM_INSTRUCTIONS}]\n",
    "    llm_models = set()\n",
    "    for chunk in input_chunks:\n",
    "        # Add chunk as user message\n",
    "        message_list.append({\"role\": \"user\", \"content\": chunk})\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            try:\n",
    "                # Get the response from the model\n",
    "                model = select_model(message_list)\n",
    "                response: ChatCompletion = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=message_list,\n",
    "                    temperature=0\n",
    "                )\n",
    "            except BadRequestError:\n",
    "                # Get the response from the model\n",
    "                model = LLM_MODELS[max(LLM_MODELS.keys())]\n",
    "                response: ChatCompletion = client.chat.completions.create(\n",
    "                    model=select_model(message_list),\n",
    "                    messages=message_list,\n",
    "                    temperature=0\n",
    "                )\n",
    "\n",
    "            # Access the response text, strip whitespace, and append as assistant message\n",
    "            message_list.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content.strip()})\n",
    "\n",
    "            # Update total cumulative cost\n",
    "            total_cumulative_cost += response.usage.prompt_tokens*MODEL_COST[model][0] + response.usage.completion_tokens*MODEL_COST[model][1]\n",
    "\n",
    "            # Add the model to the set of used models\n",
    "            llm_models.add(model)\n",
    "\n",
    "            # If not response.choices[0].finish_reason == \"length\", then we are done with the chunk\n",
    "            if response.choices[0].finish_reason == \"length\":\n",
    "                done = False\n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "    # Combine the assistant messages into a single string\n",
    "    edited_text = \" \".join([message[\"content\"] for message in message_list if message[\"role\"] == \"assistant\"])\n",
    "\n",
    "    return EditResult(edited_text=edited_text, llm_models=list(llm_models), llm_cost=total_cumulative_cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-developer-playground-6LGp-str",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
